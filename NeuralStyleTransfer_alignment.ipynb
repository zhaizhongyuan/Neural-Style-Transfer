{"cells":[{"cell_type":"markdown","id":"ffee70fc","metadata":{"heading_collapsed":true,"id":"ffee70fc"},"source":["## import packages"]},{"cell_type":"code","execution_count":null,"id":"9a817516","metadata":{"hidden":true,"id":"9a817516","outputId":"c15c8cd8-5282-4354-fd76-12a297aeb349"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import time\n","import os\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","cuda = torch.cuda.is_available()\n","device = torch.device('cuda' if cuda else 'cpu')\n","print(device)\n","\n","import torchvision\n","from torchvision import transforms\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from collections import OrderedDict\n","import numpy as np"]},{"cell_type":"markdown","id":"c10e6ef7","metadata":{"heading_collapsed":true,"id":"c10e6ef7"},"source":["## Directories and files"]},{"cell_type":"code","execution_count":null,"id":"43bdc710","metadata":{"hidden":true,"id":"43bdc710"},"outputs":[],"source":["image_dir = os.getcwd() + '/Images/'\n","model_dir = os.getcwd() + '/Models/'\n","output_dir = os.getcwd() + '/Output/'"]},{"cell_type":"markdown","id":"50d5d0f3","metadata":{"heading_collapsed":true,"id":"50d5d0f3"},"source":["## VGG NN and functions"]},{"cell_type":"code","execution_count":null,"id":"52aa5ae6","metadata":{"hidden":true,"id":"52aa5ae6"},"outputs":[],"source":["#vgg definition that conveniently let's you grab the outputs from any layer\n","class VGG(nn.Module):\n","    def __init__(self, pool='max'):\n","        super(VGG, self).__init__()\n","        #vgg modules\n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        if pool == 'max':\n","            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif pool == 'avg':\n","            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n","            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n","            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n","            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n","            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n","            \n","    def forward(self, x, out_keys):\n","        out = {}\n","        out['r11'] = F.relu(self.conv1_1(x))\n","        out['r12'] = F.relu(self.conv1_2(out['r11']))\n","        out['p1'] = self.pool1(out['r12'])\n","        out['r21'] = F.relu(self.conv2_1(out['p1']))\n","        out['r22'] = F.relu(self.conv2_2(out['r21']))\n","        out['p2'] = self.pool2(out['r22'])\n","        out['r31'] = F.relu(self.conv3_1(out['p2']))\n","        out['r32'] = F.relu(self.conv3_2(out['r31']))\n","        out['r33'] = F.relu(self.conv3_3(out['r32']))\n","        out['r34'] = F.relu(self.conv3_4(out['r33']))\n","        out['p3'] = self.pool3(out['r34'])\n","        out['r41'] = F.relu(self.conv4_1(out['p3']))\n","        out['r42'] = F.relu(self.conv4_2(out['r41']))\n","        out['r43'] = F.relu(self.conv4_3(out['r42']))\n","        out['r44'] = F.relu(self.conv4_4(out['r43']))\n","        out['p4'] = self.pool4(out['r44'])\n","        out['r51'] = F.relu(self.conv5_1(out['p4']))\n","        out['r52'] = F.relu(self.conv5_2(out['r51']))\n","        out['r53'] = F.relu(self.conv5_3(out['r52']))\n","        out['r54'] = F.relu(self.conv5_4(out['r53']))\n","        out['p5'] = self.pool5(out['r54'])\n","        return [out[key] for key in out_keys]"]},{"cell_type":"code","execution_count":null,"id":"698f6c0b","metadata":{"hidden":true,"id":"698f6c0b"},"outputs":[],"source":["'''\n","# gram matrix and loss\n","class GramMatrix(nn.Module):\n","    def forward(self, input):\n","        b,c,h,w = input.size()\n","        F = input.view(b, c, h*w)\n","        F = torch.nn.functional.normalize(F, dim = 2) #normalize the matrix, don't emphasize on bright-darkness\n","        # entries of F looks small\n","        F = F.multiply(100)\n","        G = torch.bmm(F, F.transpose(1,2)) \n","        #G.div_(h*w)\n","        return G\n","'''\n","\n","# gram matrix and loss\n","class GramMatrix(nn.Module):\n","    def forward(self, input):\n","        b,c,h,w = input.size()\n","        F = input.view(b, c, h*w)\n","        F = torch.nn.functional.normalize(F, dim = 2) #normalize the matrix, don't emphasize on bright-darkness\n","        # entries of F looks small\n","        F = F.multiply(150)\n","        G = torch.bmm(F, F.transpose(1,2)) \n","        #G.div_(h*w)\n","        return G\n","\n","class GramMSELoss(nn.Module):\n","    def forward(self, input, target):\n","        out = nn.MSELoss()(GramMatrix()(input), target)\n","        return(out)"]},{"cell_type":"markdown","id":"e3a02828","metadata":{"heading_collapsed":true,"id":"e3a02828"},"source":["## Adapt images?"]},{"cell_type":"code","execution_count":null,"id":"80cd8bf0","metadata":{"hidden":true,"id":"80cd8bf0"},"outputs":[],"source":["# pre and post processing for images\n","img_size = 512 \n","prep = transforms.Compose([transforms.Resize(img_size),\n","                           transforms.ToTensor(),\n","                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n","                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n","                                                std=[1,1,1]),\n","                           transforms.Lambda(lambda x: x.mul_(255)),\n","                          ])\n","postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n","                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n","                                                std=[1,1,1]),\n","                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n","                           ])\n","postpb = transforms.Compose([transforms.ToPILImage()])\n","def postp(tensor): # to clip results in the range [0,1]\n","    t = postpa(tensor)\n","    t[t>1] = 1    \n","    t[t<0] = 0\n","    img = postpb(t)\n","    return img"]},{"cell_type":"markdown","id":"af34640b","metadata":{"id":"af34640b"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"id":"41aacd96","metadata":{"id":"41aacd96"},"outputs":[],"source":["def run_transfer(style_name, content_name,\n","                 output_name=\"output\",\n","                 output_dir=output_dir,\n","                 pool_method=\"avg\",\n","                 style_layers=['r11','r21','r31','r41','r51'],\n","                 content_layers=['r42'],\n","                 content_weight=1e0,\n","                 style_weight=1e3,\n","                 style_layer_weights=[64,128,256,512,512], \n","                 init_method=\"content\", \n","                 max_iter=1000, \n","                 show_iter=50,\n","                 output_metrics=True):\n","\n","    # set up time and loss trackers\n","    start_time = time.time()\n","    times = []\n","    losses = []\n","\n","    #get network\n","    vgg = VGG(pool=pool_method)\n","    vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n","    for param in vgg.parameters():\n","        param.requires_grad = False\n","    if torch.cuda.is_available():\n","        vgg.cuda()\n","\n","    #load images, ordered as [style_image, content_image]\n","    img_dirs = [image_dir, image_dir]\n","    img_names = [style_name, content_name]\n","    imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n","    imgs_torch = [prep(img) for img in imgs]\n","    if torch.cuda.is_available():\n","        imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n","    else:\n","        imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n","    style_image, content_image = imgs_torch\n","\n","    #initialize the output image\n","    if init_method == \"random\":\n","        opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True)\n","    elif init_method == \"content\":\n","        opt_img = Variable(content_image.data.clone(), requires_grad=True)\n","    elif init_method == \"style\":\n","        resize_to_content = transforms.Resize((imgs[1].height, imgs[1].width))\n","        style_image_resized = resize_to_content(imgs[0])\n","        if torch.cuda.is_available():\n","            opt_img = Variable(prep(style_image_resized).unsqueeze(0).cuda(), requires_grad=True)\n","        else:\n","            opt_img = Variable(prep(style_image_resized).unsqueeze(0), requires_grad=True)\n","\n","    optimizer = optim.LBFGS([opt_img])\n","    n_iter=[0]\n","\n","    #define layers, loss functions, weights and compute optimization targets\n","    loss_layers = style_layers + content_layers\n","    loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n","    if torch.cuda.is_available():\n","        loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n","        \n","    #these are good weights settings:\n","    style_weights = [style_weight/n**2 for n in style_layer_weights]\n","    content_weights = [content_weight]\n","    weights = style_weights + content_weights\n","\n","    #compute optimization targets\n","    style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n","    content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n","    targets = style_targets + content_targets\n","\n","    #run style transfer\n","    times.append(time.time()-start_time)\n","    while n_iter[0] <= max_iter:\n","\n","        def closure():\n","            optimizer.zero_grad()\n","            out = vgg(opt_img, loss_layers)\n","            layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n","            loss = torch.stack(layer_losses, dim=0).sum(dim=0)\n","            loss.backward()\n","            n_iter[0]+=1\n","            if n_iter[0]%show_iter == (show_iter-1):\n","                print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n","                losses.append(loss.item())\n","                times.append(time.time()-start_time)\n","            return loss\n","        \n","        optimizer.step(closure)\n","        \n","    #display result\n","    out_img = postp(opt_img.data[0].cpu().squeeze())\n","    out_img.save(output_dir + output_name + \".png\")\n","    if output_metrics: return losses, times"]},{"cell_type":"markdown","id":"3ed777d3","metadata":{"id":"3ed777d3"},"source":["## Running code"]},{"cell_type":"markdown","id":"b4017686","metadata":{"id":"b4017686"},"source":["style_name, content_name, variation, style_layers, content_layers, style_weights, init_method=\"random\", max_iter=500, show_iter=50, output_dir=output_dir"]},{"cell_type":"code","execution_count":null,"id":"57c9c060","metadata":{"id":"57c9c060"},"outputs":[],"source":["content_images = [\"Content/10.png\" for i in range(10)]\n","style_images = [\"Style/01.png\", \"Style/02.png\", \"Style/03.png\", \"Style/04.png\", \"Style/05.png\", \n","                \"Style/06.png\", \"Style/07.png\", \"Style/08.png\", \"Style/09.png\", \"Style/10.png\"]"]},{"cell_type":"code","execution_count":null,"id":"9a66a166","metadata":{"id":"9a66a166","outputId":"aa3e3272-1cdc-4c72-a6ec-370c9fc9ef2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 50, loss: 53583.304688\n","Iteration: 50, loss: 54582.390625\n","Iteration: 50, loss: 50437.375000\n","Iteration: 50, loss: 59566.734375\n","Iteration: 50, loss: 53388.316406\n","Iteration: 50, loss: 39823.324219\n","Iteration: 50, loss: 39757.039062\n","Iteration: 50, loss: 63783.648438\n","Iteration: 50, loss: 75314.265625\n","Iteration: 50, loss: 99806.093750\n"]}],"source":["align_losses, align_times = list(), list()\n","for i in range(len(content_images)):\n","    l,t = run_transfer(style_name = style_images[i], \n","                       content_name = content_images[i],\n","                       max_iter = 2000, \n","                       output_name = \"s\" + style_images[i][6:8] + \"_align\",\n","                       output_dir = output_dir + \"portrait_aware_10/\",\n","                       output_metrics = True)\n","    align_losses.append(l)\n","    align_times.append(t)"]},{"cell_type":"code","execution_count":null,"id":"a7d157bc","metadata":{"id":"a7d157bc"},"outputs":[],"source":["align_losses = np.array(align_losses)\n","align_times = np.array(align_times)\n","\n","np.savetxt(output_dir+\"portrait_align_losses.csv\", align_losses, delimiter=\",\")\n","np.savetxt(output_dir+\"portrait_align_times.csv\", align_times, delimiter=\",\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"NeuralStyleTransfer_alignment.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}