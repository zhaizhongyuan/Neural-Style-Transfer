{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgBJi5f5G1Zk",
        "outputId": "612b25bc-c3e1-4bd4-f7c0-7fc069ca882a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXdlA6nHIJVO",
        "outputId": "9519013b-2cde-4592-e8e8-7b1c972dc0fa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "image_dir = os.getcwd() + '/Images/'\n",
        "model_dir = os.getcwd() + '/Models/'\n",
        "output_dir = os.getcwd() + '/Output/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Z-P_kQM_EJ0S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "print(device)\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_images = [\"Content/04.png\", \"Content/01.png\", \"Content/05.png\", \"Content/10.png\", \"Content/12.png\", \"Content/19.png\"]\n",
        "style_images = [\"Style/01.png\", \"Style/02.png\", \"Style/03.png\", \"Style/10.png\", \"Style/04.png\", \"Style/09.png\"]\n",
        "n_iterations = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": true,
        "id": "Ut1iHi6kEJ0U"
      },
      "outputs": [],
      "source": [
        "#vgg definition that conveniently let's you grab the outputs from any layer\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, pool='max'):\n",
        "        super(VGG, self).__init__()\n",
        "        #vgg modules\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        if pool == 'max':\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pool == 'avg':\n",
        "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "            \n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        out['r11'] = F.relu(self.conv1_1(x))\n",
        "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
        "        out['p1'] = self.pool1(out['r12'])\n",
        "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
        "        out['p2'] = self.pool2(out['r22'])\n",
        "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
        "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
        "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
        "        out['p3'] = self.pool3(out['r34'])\n",
        "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
        "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
        "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
        "        out['p4'] = self.pool4(out['r44'])\n",
        "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
        "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
        "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
        "        out['p5'] = self.pool5(out['r54'])\n",
        "        return [out[key] for key in out_keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "brzisSSLEJ0V"
      },
      "outputs": [],
      "source": [
        "# gram matrix and loss\n",
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b,c,h,w = input.size()\n",
        "        F = input.view(b, c, h*w)\n",
        "        G = torch.bmm(F, F.transpose(1,2)) \n",
        "        G.div_(h*w)\n",
        "        return G\n",
        "\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
        "        return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "R1THuFExEJ0V"
      },
      "outputs": [],
      "source": [
        "# pre and post processing for images\n",
        "img_size = 512 \n",
        "prep = transforms.Compose([transforms.Resize(img_size),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
        "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
        "                          ])\n",
        "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
        "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
        "                                                std=[1,1,1]),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
        "                           ])\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor): # to clip results in the range [0,1]\n",
        "    t = postpa(tensor)\n",
        "    t[t>1] = 1    \n",
        "    t[t<0] = 0\n",
        "    img = postpb(t)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IPymFEN7EJ0W"
      },
      "outputs": [],
      "source": [
        "#get network\n",
        "vgg = VGG(pool=\"avg\")\n",
        "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "if torch.cuda.is_available():\n",
        "    vgg.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "t5K1UiJLEJ0X"
      },
      "outputs": [],
      "source": [
        "def run_transfer(style_name, content_name, init_method=\"content\", max_iter=500, show_iter=50, output_dir=output_dir):\n",
        "    #load images, ordered as [style_image, content_image]\n",
        "    img_dirs = [image_dir, image_dir]\n",
        "    img_names = [style_name, content_name]\n",
        "    imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
        "    imgs_torch = [prep(img) for img in imgs]\n",
        "    if torch.cuda.is_available():\n",
        "        imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
        "    else:\n",
        "        imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
        "    style_image, content_image = imgs_torch\n",
        "\n",
        "    if init_method == \"random\":\n",
        "        opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True)\n",
        "    elif init_method == \"content\":\n",
        "        opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
        "    elif init_method == \"style\":\n",
        "        resize_to_content = transforms.Resize((imgs[1].height, imgs[1].width))\n",
        "        style_image_resized = resize_to_content(imgs[0])\n",
        "        if torch.cuda.is_available():\n",
        "            opt_img = Variable(prep(style_image_resized).unsqueeze(0).cuda(), requires_grad=True)\n",
        "        else:\n",
        "            opt_img = Variable(prep(style_image_resized).unsqueeze(0), requires_grad=True)\n",
        "\n",
        "    optimizer = optim.LBFGS([opt_img])\n",
        "    n_iter=[0]\n",
        "\n",
        "    #define layers, loss functions, weights and compute optimization targets\n",
        "    style_layers = ['r11','r21','r31','r41','r51'] \n",
        "    content_layers = ['r42']\n",
        "    loss_layers = style_layers + content_layers\n",
        "    loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
        "    if torch.cuda.is_available():\n",
        "        loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
        "        \n",
        "    #these are good weights settings:\n",
        "    style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
        "    content_weights = [1e0]\n",
        "    weights = style_weights + content_weights\n",
        "\n",
        "    #compute optimization targets\n",
        "    style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
        "    content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
        "    targets = style_targets + content_targets\n",
        "\n",
        "    # set up timer\n",
        "    losses = []\n",
        "\n",
        "    #run style transfer\n",
        "    while n_iter[0] <= max_iter:\n",
        "\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            out = vgg(opt_img, loss_layers)\n",
        "            layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
        "            loss = torch.stack(layer_losses, dim=0).sum(dim=0)\n",
        "            loss.backward()\n",
        "            n_iter[0]+=1\n",
        "            if n_iter[0]%show_iter == (show_iter-1):\n",
        "                print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
        "                losses.append(loss.item())\n",
        "            return loss\n",
        "        \n",
        "        optimizer.step(closure)\n",
        "        \n",
        "    #display result\n",
        "    out_img = postp(opt_img.data[0].cpu().squeeze())\n",
        "    #plt.imshow(out_img)\n",
        "    #plt.gcf().set_size_inches(10,10)\n",
        "    out_img.save(output_dir + \"c\" + content_name[8:10] + \"_s\" + style_name[6:8] + \".png\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\MSI-NB\\Desktop\\2022_Spring\\10-701\\project\\Neural-Style-Transfer\\NeuralStyleTransfer_avg_pool.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000009?line=0'>1</a>\u001b[0m init_content_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(style_images)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000009?line=2'>3</a>\u001b[0m     init_content_losses\u001b[39m.\u001b[39mappend(run_transfer(style_images[i], content_images[i], \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m2000\u001b[39;49m, \u001b[39m25\u001b[39;49m, output_dir\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcontent_avg_2000/\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
            "\u001b[1;32mc:\\Users\\MSI-NB\\Desktop\\2022_Spring\\10-701\\project\\Neural-Style-Transfer\\NeuralStyleTransfer_avg_pool.ipynb Cell 9'\u001b[0m in \u001b[0;36mrun_transfer\u001b[1;34m(style_name, content_name, init_method, max_iter, show_iter, output_dir)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000008?line=60'>61</a>\u001b[0m             losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000008?line=61'>62</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000008?line=63'>64</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000008?line=65'>66</a>\u001b[0m \u001b[39m#display result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI-NB/Desktop/2022_Spring/10-701/project/Neural-Style-Transfer/NeuralStyleTransfer_avg_pool.ipynb#ch0000008?line=66'>67</a>\u001b[0m out_img \u001b[39m=\u001b[39m postp(opt_img\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze())\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\optim\\lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=432'>433</a>\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=433'>434</a>\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=434'>435</a>\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=435'>436</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=436'>437</a>\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m(closure())\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=437'>438</a>\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    <a href='file:///c%3A/Users/MSI-NB/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/torch/optim/lbfgs.py?line=438'>439</a>\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "content_losses = []\n",
        "for i in range(len(style_images)):\n",
        "    content_losses.append(run_transfer(style_images[i], content_images[i], \"content\", 2000, 25, output_dir+'content_avg_2000/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 1395631.500000\n",
            "Iteration: 50, loss: 321296.625000\n",
            "Iteration: 75, loss: 191417.718750\n",
            "Iteration: 100, loss: 138388.187500\n",
            "Iteration: 125, loss: 111357.507812\n",
            "Iteration: 150, loss: 92122.773438\n",
            "Iteration: 175, loss: 78869.171875\n",
            "Iteration: 200, loss: 69861.125000\n",
            "Iteration: 225, loss: 64171.406250\n",
            "Iteration: 250, loss: 59870.042969\n",
            "Iteration: 275, loss: 56866.125000\n",
            "Iteration: 300, loss: 54556.960938\n",
            "Iteration: 325, loss: 52766.007812\n",
            "Iteration: 350, loss: 51334.359375\n",
            "Iteration: 375, loss: 50116.921875\n",
            "Iteration: 400, loss: 49044.082031\n",
            "Iteration: 425, loss: 48051.566406\n",
            "Iteration: 450, loss: 47177.195312\n",
            "Iteration: 475, loss: 46418.789062\n",
            "Iteration: 500, loss: 45722.460938\n",
            "Iteration: 525, loss: 45095.921875\n",
            "Iteration: 550, loss: 44512.765625\n",
            "Iteration: 575, loss: 43992.617188\n",
            "Iteration: 600, loss: 43519.937500\n",
            "Iteration: 625, loss: 43121.542969\n",
            "Iteration: 650, loss: 42749.445312\n",
            "Iteration: 675, loss: 42435.296875\n",
            "Iteration: 700, loss: 42146.957031\n",
            "Iteration: 725, loss: 41873.687500\n",
            "Iteration: 750, loss: 41630.683594\n",
            "Iteration: 775, loss: 41413.734375\n",
            "Iteration: 800, loss: 41213.851562\n",
            "Iteration: 825, loss: 41045.457031\n",
            "Iteration: 850, loss: 40884.773438\n",
            "Iteration: 875, loss: 40732.511719\n",
            "Iteration: 900, loss: 40593.621094\n",
            "Iteration: 925, loss: 40470.628906\n",
            "Iteration: 950, loss: 40356.109375\n",
            "Iteration: 975, loss: 40253.652344\n",
            "Iteration: 1000, loss: 40148.085938\n",
            "Iteration: 1025, loss: 40059.726562\n",
            "Iteration: 1050, loss: 39979.566406\n",
            "Iteration: 1075, loss: 39904.042969\n",
            "Iteration: 1100, loss: 39828.207031\n",
            "Iteration: 1125, loss: 39753.125000\n",
            "Iteration: 1150, loss: 39691.351562\n",
            "Iteration: 1175, loss: 39632.902344\n",
            "Iteration: 1200, loss: 39575.574219\n",
            "Iteration: 1225, loss: 39524.300781\n",
            "Iteration: 1250, loss: 39480.253906\n",
            "Iteration: 1275, loss: 39436.722656\n",
            "Iteration: 1300, loss: 39398.984375\n",
            "Iteration: 1325, loss: 39356.457031\n",
            "Iteration: 1350, loss: 39320.445312\n",
            "Iteration: 1375, loss: 39285.972656\n",
            "Iteration: 1400, loss: 39252.273438\n",
            "Iteration: 1425, loss: 39219.167969\n",
            "Iteration: 1450, loss: 39189.078125\n",
            "Iteration: 1475, loss: 39158.187500\n",
            "Iteration: 1500, loss: 39128.425781\n",
            "Iteration: 1525, loss: 39100.039062\n",
            "Iteration: 1550, loss: 39072.578125\n",
            "Iteration: 1575, loss: 39048.128906\n",
            "Iteration: 1600, loss: 39022.714844\n",
            "Iteration: 1625, loss: 38999.132812\n",
            "Iteration: 1650, loss: 38977.035156\n",
            "Iteration: 1675, loss: 38957.230469\n",
            "Iteration: 1700, loss: 38940.683594\n",
            "Iteration: 1725, loss: 38921.332031\n",
            "Iteration: 1750, loss: 38904.144531\n",
            "Iteration: 1775, loss: 38888.828125\n",
            "Iteration: 1800, loss: 38874.050781\n",
            "Iteration: 1825, loss: 38859.964844\n",
            "Iteration: 1850, loss: 38845.351562\n",
            "Iteration: 1875, loss: 38832.890625\n",
            "Iteration: 1900, loss: 38820.324219\n",
            "Iteration: 1925, loss: 38808.457031\n",
            "Iteration: 1950, loss: 38796.886719\n",
            "Iteration: 1975, loss: 38784.625000\n",
            "Iteration: 2000, loss: 38773.902344\n",
            "Iteration: 25, loss: 4701710.500000\n",
            "Iteration: 50, loss: 870869.625000\n",
            "Iteration: 75, loss: 229841.156250\n",
            "Iteration: 100, loss: 91718.093750\n",
            "Iteration: 125, loss: 61330.496094\n",
            "Iteration: 150, loss: 49278.078125\n",
            "Iteration: 175, loss: 42617.265625\n",
            "Iteration: 200, loss: 38331.394531\n",
            "Iteration: 225, loss: 34986.992188\n",
            "Iteration: 250, loss: 32467.824219\n",
            "Iteration: 275, loss: 30477.902344\n",
            "Iteration: 300, loss: 28879.798828\n",
            "Iteration: 325, loss: 27542.902344\n",
            "Iteration: 350, loss: 26413.445312\n",
            "Iteration: 375, loss: 25457.054688\n",
            "Iteration: 400, loss: 24660.503906\n",
            "Iteration: 425, loss: 23983.740234\n",
            "Iteration: 450, loss: 23395.673828\n",
            "Iteration: 475, loss: 22858.425781\n",
            "Iteration: 500, loss: 22438.689453\n",
            "Iteration: 525, loss: 22048.248047\n",
            "Iteration: 550, loss: 21725.455078\n",
            "Iteration: 575, loss: 21458.679688\n",
            "Iteration: 600, loss: 21236.765625\n",
            "Iteration: 625, loss: 21044.707031\n",
            "Iteration: 650, loss: 20883.787109\n",
            "Iteration: 675, loss: 20734.115234\n",
            "Iteration: 700, loss: 20598.289062\n",
            "Iteration: 725, loss: 20479.037109\n",
            "Iteration: 750, loss: 20367.939453\n",
            "Iteration: 775, loss: 20266.750000\n",
            "Iteration: 800, loss: 20171.148438\n",
            "Iteration: 825, loss: 20085.789062\n",
            "Iteration: 850, loss: 20007.855469\n",
            "Iteration: 875, loss: 19937.402344\n",
            "Iteration: 900, loss: 19874.271484\n",
            "Iteration: 925, loss: 19820.148438\n",
            "Iteration: 950, loss: 19773.300781\n",
            "Iteration: 975, loss: 19733.369141\n",
            "Iteration: 1000, loss: 19695.101562\n",
            "Iteration: 1025, loss: 19655.966797\n",
            "Iteration: 1050, loss: 19622.650391\n",
            "Iteration: 1075, loss: 19589.304688\n",
            "Iteration: 1100, loss: 19559.517578\n",
            "Iteration: 1125, loss: 19532.279297\n",
            "Iteration: 1150, loss: 19506.195312\n",
            "Iteration: 1175, loss: 19483.007812\n",
            "Iteration: 1200, loss: 19460.332031\n",
            "Iteration: 1225, loss: 19437.597656\n",
            "Iteration: 1250, loss: 19418.960938\n",
            "Iteration: 1275, loss: 19399.923828\n",
            "Iteration: 1300, loss: 19382.308594\n",
            "Iteration: 1325, loss: 19364.646484\n",
            "Iteration: 1350, loss: 19349.105469\n",
            "Iteration: 1375, loss: 19336.029297\n",
            "Iteration: 1400, loss: 19322.244141\n",
            "Iteration: 1425, loss: 19309.126953\n",
            "Iteration: 1450, loss: 19296.865234\n",
            "Iteration: 1475, loss: 19285.900391\n",
            "Iteration: 1500, loss: 19275.972656\n",
            "Iteration: 1525, loss: 19263.750000\n",
            "Iteration: 1550, loss: 19253.664062\n",
            "Iteration: 1575, loss: 19244.134766\n",
            "Iteration: 1600, loss: 19235.470703\n",
            "Iteration: 1625, loss: 19227.152344\n",
            "Iteration: 1650, loss: 19219.449219\n",
            "Iteration: 1675, loss: 19212.656250\n",
            "Iteration: 1700, loss: 19206.734375\n",
            "Iteration: 1725, loss: 19201.189453\n",
            "Iteration: 1750, loss: 19195.406250\n",
            "Iteration: 1775, loss: 19190.498047\n",
            "Iteration: 1800, loss: 19185.947266\n",
            "Iteration: 1825, loss: 19181.558594\n",
            "Iteration: 1850, loss: 19176.544922\n",
            "Iteration: 1875, loss: 19172.474609\n",
            "Iteration: 1900, loss: 19168.636719\n",
            "Iteration: 1925, loss: 19165.023438\n",
            "Iteration: 1950, loss: 19160.683594\n",
            "Iteration: 1975, loss: 19157.277344\n",
            "Iteration: 2000, loss: 19153.832031\n",
            "Iteration: 25, loss: 8346492.000000\n",
            "Iteration: 50, loss: 2407156.000000\n",
            "Iteration: 75, loss: 1128294.250000\n",
            "Iteration: 100, loss: 617766.875000\n",
            "Iteration: 125, loss: 428185.187500\n",
            "Iteration: 150, loss: 337654.593750\n",
            "Iteration: 175, loss: 280689.968750\n",
            "Iteration: 200, loss: 244703.390625\n",
            "Iteration: 225, loss: 219129.765625\n",
            "Iteration: 250, loss: 199530.453125\n",
            "Iteration: 275, loss: 184990.234375\n",
            "Iteration: 300, loss: 173794.921875\n",
            "Iteration: 325, loss: 164227.718750\n",
            "Iteration: 350, loss: 156879.765625\n",
            "Iteration: 375, loss: 150223.937500\n",
            "Iteration: 400, loss: 144540.968750\n",
            "Iteration: 425, loss: 139727.281250\n",
            "Iteration: 450, loss: 135465.765625\n",
            "Iteration: 475, loss: 131564.390625\n",
            "Iteration: 500, loss: 128535.742188\n",
            "Iteration: 525, loss: 125538.968750\n",
            "Iteration: 550, loss: 122794.070312\n",
            "Iteration: 575, loss: 120289.250000\n",
            "Iteration: 600, loss: 118085.546875\n",
            "Iteration: 625, loss: 115864.507812\n",
            "Iteration: 650, loss: 114006.812500\n",
            "Iteration: 675, loss: 112312.093750\n",
            "Iteration: 700, loss: 110733.914062\n",
            "Iteration: 725, loss: 109101.796875\n",
            "Iteration: 750, loss: 107626.343750\n",
            "Iteration: 775, loss: 106277.203125\n",
            "Iteration: 800, loss: 104960.585938\n",
            "Iteration: 825, loss: 103711.796875\n",
            "Iteration: 850, loss: 102612.843750\n",
            "Iteration: 875, loss: 101526.148438\n",
            "Iteration: 900, loss: 100480.789062\n",
            "Iteration: 925, loss: 99544.203125\n",
            "Iteration: 950, loss: 98664.187500\n",
            "Iteration: 975, loss: 97858.468750\n",
            "Iteration: 1000, loss: 97025.218750\n",
            "Iteration: 1025, loss: 96317.562500\n",
            "Iteration: 1050, loss: 95628.875000\n",
            "Iteration: 1075, loss: 95018.390625\n",
            "Iteration: 1100, loss: 94307.218750\n",
            "Iteration: 1125, loss: 93709.625000\n",
            "Iteration: 1150, loss: 93117.656250\n",
            "Iteration: 1175, loss: 92573.695312\n",
            "Iteration: 1200, loss: 91979.406250\n",
            "Iteration: 1225, loss: 91436.882812\n",
            "Iteration: 1250, loss: 90921.750000\n",
            "Iteration: 1275, loss: 90438.523438\n",
            "Iteration: 1300, loss: 89968.148438\n",
            "Iteration: 1325, loss: 89532.289062\n",
            "Iteration: 1350, loss: 89091.125000\n",
            "Iteration: 1375, loss: 88692.820312\n",
            "Iteration: 1400, loss: 88290.460938\n",
            "Iteration: 1425, loss: 87889.132812\n",
            "Iteration: 1450, loss: 87508.773438\n",
            "Iteration: 1475, loss: 87166.062500\n",
            "Iteration: 1500, loss: 86832.132812\n",
            "Iteration: 1525, loss: 86500.976562\n",
            "Iteration: 1550, loss: 86183.765625\n",
            "Iteration: 1575, loss: 85885.804688\n",
            "Iteration: 1600, loss: 85592.960938\n",
            "Iteration: 1625, loss: 85311.132812\n",
            "Iteration: 1650, loss: 85030.054688\n",
            "Iteration: 1675, loss: 84768.671875\n",
            "Iteration: 1700, loss: 84527.101562\n",
            "Iteration: 1725, loss: 84258.273438\n",
            "Iteration: 1750, loss: 84022.062500\n",
            "Iteration: 1775, loss: 83776.382812\n",
            "Iteration: 1800, loss: 83521.898438\n",
            "Iteration: 1825, loss: 83259.203125\n",
            "Iteration: 1850, loss: 83038.523438\n",
            "Iteration: 1875, loss: 82818.945312\n",
            "Iteration: 1900, loss: 82604.796875\n",
            "Iteration: 1925, loss: 82383.015625\n",
            "Iteration: 1950, loss: 82163.328125\n",
            "Iteration: 1975, loss: 81978.164062\n",
            "Iteration: 2000, loss: 81771.273438\n",
            "Iteration: 25, loss: 821970432.000000\n",
            "Iteration: 50, loss: 5207108.500000\n",
            "Iteration: 75, loss: 2526592.750000\n",
            "Iteration: 100, loss: 1405463.125000\n",
            "Iteration: 125, loss: 869355.750000\n",
            "Iteration: 150, loss: 583348.562500\n",
            "Iteration: 175, loss: 415132.375000\n",
            "Iteration: 200, loss: 318303.375000\n",
            "Iteration: 225, loss: 261176.484375\n",
            "Iteration: 250, loss: 222532.078125\n",
            "Iteration: 275, loss: 197436.015625\n",
            "Iteration: 300, loss: 177048.781250\n",
            "Iteration: 325, loss: 164826.531250\n",
            "Iteration: 350, loss: 152032.875000\n",
            "Iteration: 375, loss: 142350.203125\n",
            "Iteration: 400, loss: 133750.265625\n",
            "Iteration: 425, loss: 126481.796875\n",
            "Iteration: 450, loss: 120900.195312\n",
            "Iteration: 475, loss: 116345.546875\n",
            "Iteration: 500, loss: 112260.593750\n",
            "Iteration: 525, loss: 108827.093750\n",
            "Iteration: 550, loss: 105554.656250\n",
            "Iteration: 575, loss: 102581.906250\n",
            "Iteration: 600, loss: 99549.101562\n",
            "Iteration: 625, loss: 96612.984375\n",
            "Iteration: 650, loss: 94542.859375\n",
            "Iteration: 675, loss: 92638.695312\n",
            "Iteration: 700, loss: 90897.523438\n",
            "Iteration: 725, loss: 89475.320312\n",
            "Iteration: 750, loss: 88187.820312\n",
            "Iteration: 775, loss: 86961.445312\n",
            "Iteration: 800, loss: 85837.710938\n",
            "Iteration: 825, loss: 84754.242188\n",
            "Iteration: 850, loss: 83692.562500\n",
            "Iteration: 875, loss: 82763.750000\n",
            "Iteration: 900, loss: 81868.570312\n",
            "Iteration: 925, loss: 81039.203125\n",
            "Iteration: 950, loss: 80260.890625\n",
            "Iteration: 975, loss: 79527.734375\n",
            "Iteration: 1000, loss: 78950.375000\n",
            "Iteration: 1025, loss: 78324.421875\n",
            "Iteration: 1050, loss: 77786.468750\n",
            "Iteration: 1075, loss: 77224.882812\n",
            "Iteration: 1100, loss: 76743.117188\n",
            "Iteration: 1125, loss: 76257.929688\n",
            "Iteration: 1150, loss: 75838.664062\n",
            "Iteration: 1175, loss: 75447.445312\n",
            "Iteration: 1200, loss: 75108.007812\n",
            "Iteration: 1225, loss: 74699.375000\n",
            "Iteration: 1250, loss: 74350.609375\n",
            "Iteration: 1275, loss: 74006.343750\n",
            "Iteration: 1300, loss: 73666.976562\n",
            "Iteration: 1325, loss: 73331.593750\n",
            "Iteration: 1350, loss: 73025.406250\n",
            "Iteration: 1375, loss: 72726.093750\n",
            "Iteration: 1400, loss: 72455.281250\n",
            "Iteration: 1425, loss: 72159.804688\n",
            "Iteration: 1450, loss: 71899.171875\n",
            "Iteration: 1475, loss: 71635.929688\n",
            "Iteration: 1500, loss: 71379.531250\n",
            "Iteration: 1525, loss: 71137.453125\n",
            "Iteration: 1550, loss: 70902.382812\n",
            "Iteration: 1575, loss: 70698.328125\n",
            "Iteration: 1600, loss: 70457.828125\n",
            "Iteration: 1625, loss: 70204.125000\n",
            "Iteration: 1650, loss: 69981.093750\n",
            "Iteration: 1675, loss: 69695.710938\n",
            "Iteration: 1700, loss: 69460.117188\n",
            "Iteration: 1725, loss: 69260.359375\n",
            "Iteration: 1750, loss: 69173.968750\n",
            "Iteration: 1775, loss: 68896.554688\n",
            "Iteration: 1800, loss: 68702.265625\n",
            "Iteration: 1825, loss: 68519.757812\n",
            "Iteration: 1850, loss: 68350.335938\n",
            "Iteration: 1875, loss: 68178.937500\n",
            "Iteration: 1900, loss: 67996.515625\n",
            "Iteration: 1925, loss: 67847.265625\n",
            "Iteration: 1950, loss: 67688.570312\n",
            "Iteration: 1975, loss: 67538.539062\n",
            "Iteration: 2000, loss: 67412.015625\n",
            "Iteration: 25, loss: 5094381.500000\n",
            "Iteration: 50, loss: 1814635.000000\n",
            "Iteration: 75, loss: 966202.875000\n",
            "Iteration: 100, loss: 610332.062500\n",
            "Iteration: 125, loss: 434675.625000\n",
            "Iteration: 150, loss: 333634.312500\n",
            "Iteration: 175, loss: 263461.593750\n",
            "Iteration: 200, loss: 217929.968750\n",
            "Iteration: 225, loss: 183970.250000\n",
            "Iteration: 250, loss: 160781.281250\n",
            "Iteration: 275, loss: 142967.125000\n",
            "Iteration: 300, loss: 131042.164062\n",
            "Iteration: 325, loss: 121728.507812\n",
            "Iteration: 350, loss: 114515.453125\n",
            "Iteration: 375, loss: 108981.468750\n",
            "Iteration: 400, loss: 104565.203125\n",
            "Iteration: 425, loss: 100497.804688\n",
            "Iteration: 450, loss: 97284.750000\n",
            "Iteration: 475, loss: 94384.125000\n",
            "Iteration: 500, loss: 91793.562500\n",
            "Iteration: 525, loss: 89586.390625\n",
            "Iteration: 550, loss: 87644.773438\n",
            "Iteration: 575, loss: 85854.617188\n",
            "Iteration: 600, loss: 84521.281250\n",
            "Iteration: 625, loss: 83248.296875\n",
            "Iteration: 650, loss: 82035.437500\n",
            "Iteration: 675, loss: 80862.960938\n",
            "Iteration: 700, loss: 79906.085938\n",
            "Iteration: 725, loss: 78943.835938\n",
            "Iteration: 750, loss: 78108.843750\n",
            "Iteration: 775, loss: 77291.992188\n",
            "Iteration: 800, loss: 76548.250000\n",
            "Iteration: 825, loss: 75829.539062\n",
            "Iteration: 850, loss: 75149.335938\n",
            "Iteration: 875, loss: 74474.812500\n",
            "Iteration: 900, loss: 73799.742188\n",
            "Iteration: 925, loss: 73137.890625\n",
            "Iteration: 950, loss: 72545.906250\n",
            "Iteration: 975, loss: 71991.453125\n",
            "Iteration: 1000, loss: 71435.226562\n",
            "Iteration: 1025, loss: 70916.906250\n",
            "Iteration: 1050, loss: 70425.421875\n",
            "Iteration: 1075, loss: 69945.703125\n",
            "Iteration: 1100, loss: 69488.218750\n",
            "Iteration: 1125, loss: 69055.031250\n",
            "Iteration: 1150, loss: 68649.203125\n",
            "Iteration: 1175, loss: 68257.921875\n",
            "Iteration: 1200, loss: 67874.203125\n",
            "Iteration: 1225, loss: 67507.437500\n",
            "Iteration: 1250, loss: 67175.265625\n",
            "Iteration: 1275, loss: 66828.437500\n",
            "Iteration: 1300, loss: 66509.851562\n",
            "Iteration: 1325, loss: 66198.859375\n",
            "Iteration: 1350, loss: 65880.625000\n",
            "Iteration: 1375, loss: 65580.460938\n",
            "Iteration: 1400, loss: 65295.500000\n",
            "Iteration: 1425, loss: 65016.152344\n",
            "Iteration: 1450, loss: 64754.214844\n",
            "Iteration: 1475, loss: 64507.457031\n",
            "Iteration: 1500, loss: 64262.429688\n",
            "Iteration: 1525, loss: 64039.835938\n",
            "Iteration: 1550, loss: 63794.242188\n",
            "Iteration: 1575, loss: 63570.089844\n",
            "Iteration: 1600, loss: 63362.390625\n",
            "Iteration: 1625, loss: 63160.031250\n",
            "Iteration: 1650, loss: 62960.531250\n",
            "Iteration: 1675, loss: 62754.101562\n",
            "Iteration: 1700, loss: 62557.796875\n",
            "Iteration: 1725, loss: 62379.664062\n",
            "Iteration: 1750, loss: 62214.207031\n",
            "Iteration: 1775, loss: 62047.777344\n",
            "Iteration: 1800, loss: 61884.867188\n",
            "Iteration: 1825, loss: 61710.570312\n",
            "Iteration: 1850, loss: 61570.882812\n",
            "Iteration: 1875, loss: 61395.921875\n",
            "Iteration: 1900, loss: 61237.140625\n",
            "Iteration: 1925, loss: 61076.996094\n",
            "Iteration: 1950, loss: 60938.464844\n",
            "Iteration: 1975, loss: 60790.480469\n",
            "Iteration: 2000, loss: 60629.343750\n",
            "Iteration: 25, loss: 6901241.000000\n",
            "Iteration: 50, loss: 845606.750000\n",
            "Iteration: 75, loss: 237900.218750\n",
            "Iteration: 100, loss: 117228.304688\n",
            "Iteration: 125, loss: 86954.062500\n",
            "Iteration: 150, loss: 74739.312500\n",
            "Iteration: 175, loss: 67413.273438\n",
            "Iteration: 200, loss: 62646.156250\n",
            "Iteration: 225, loss: 59161.292969\n",
            "Iteration: 250, loss: 56711.261719\n",
            "Iteration: 275, loss: 54931.597656\n",
            "Iteration: 300, loss: 53514.675781\n",
            "Iteration: 325, loss: 52437.734375\n",
            "Iteration: 350, loss: 51529.882812\n",
            "Iteration: 375, loss: 50749.148438\n",
            "Iteration: 400, loss: 50100.949219\n",
            "Iteration: 425, loss: 49497.582031\n",
            "Iteration: 450, loss: 48991.617188\n",
            "Iteration: 475, loss: 48532.699219\n",
            "Iteration: 500, loss: 48122.527344\n",
            "Iteration: 525, loss: 47747.035156\n",
            "Iteration: 550, loss: 47413.472656\n",
            "Iteration: 575, loss: 47130.992188\n",
            "Iteration: 600, loss: 46869.359375\n",
            "Iteration: 625, loss: 46644.058594\n",
            "Iteration: 650, loss: 46446.257812\n",
            "Iteration: 675, loss: 46268.546875\n",
            "Iteration: 700, loss: 46111.429688\n",
            "Iteration: 725, loss: 45978.484375\n",
            "Iteration: 750, loss: 45855.375000\n",
            "Iteration: 775, loss: 45751.878906\n",
            "Iteration: 800, loss: 45660.972656\n",
            "Iteration: 825, loss: 45576.906250\n",
            "Iteration: 850, loss: 45504.269531\n",
            "Iteration: 875, loss: 45436.269531\n",
            "Iteration: 900, loss: 45380.621094\n",
            "Iteration: 925, loss: 45325.617188\n",
            "Iteration: 950, loss: 45273.207031\n",
            "Iteration: 975, loss: 45224.304688\n",
            "Iteration: 1000, loss: 45184.382812\n",
            "Iteration: 1025, loss: 45146.250000\n",
            "Iteration: 1050, loss: 45107.472656\n",
            "Iteration: 1075, loss: 45071.199219\n",
            "Iteration: 1100, loss: 45037.925781\n",
            "Iteration: 1125, loss: 45005.687500\n",
            "Iteration: 1150, loss: 44975.066406\n",
            "Iteration: 1175, loss: 44943.300781\n",
            "Iteration: 1200, loss: 44916.382812\n",
            "Iteration: 1225, loss: 44888.914062\n",
            "Iteration: 1250, loss: 44861.546875\n",
            "Iteration: 1275, loss: 44835.335938\n",
            "Iteration: 1300, loss: 44813.289062\n",
            "Iteration: 1325, loss: 44793.753906\n",
            "Iteration: 1350, loss: 44772.339844\n",
            "Iteration: 1375, loss: 44749.304688\n",
            "Iteration: 1400, loss: 44731.066406\n",
            "Iteration: 1425, loss: 44711.898438\n",
            "Iteration: 1450, loss: 44691.875000\n",
            "Iteration: 1475, loss: 44672.019531\n",
            "Iteration: 1500, loss: 44653.476562\n",
            "Iteration: 1525, loss: 44638.570312\n",
            "Iteration: 1550, loss: 44621.480469\n",
            "Iteration: 1575, loss: 44603.929688\n",
            "Iteration: 1600, loss: 44588.007812\n",
            "Iteration: 1625, loss: 44576.046875\n",
            "Iteration: 1650, loss: 44564.019531\n",
            "Iteration: 1675, loss: 44551.402344\n",
            "Iteration: 1700, loss: 44538.671875\n",
            "Iteration: 1725, loss: 44528.097656\n",
            "Iteration: 1750, loss: 44518.382812\n",
            "Iteration: 1775, loss: 44507.812500\n",
            "Iteration: 1800, loss: 44497.523438\n",
            "Iteration: 1825, loss: 44489.320312\n",
            "Iteration: 1850, loss: 44481.042969\n",
            "Iteration: 1875, loss: 44472.468750\n",
            "Iteration: 1900, loss: 44464.058594\n",
            "Iteration: 1925, loss: 44455.847656\n",
            "Iteration: 1950, loss: 44448.542969\n",
            "Iteration: 1975, loss: 44441.882812\n",
            "Iteration: 2000, loss: 44434.777344\n"
          ]
        }
      ],
      "source": [
        "random_losses = []\n",
        "for i in range(len(style_images)):\n",
        "    random_losses.append(run_transfer(style_images[i], content_images[i], \"random\", 2000, 25, output_dir+'random_avg_2000/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 96098.789062\n",
            "Iteration: 50, loss: 73704.390625\n",
            "Iteration: 75, loss: 66202.617188\n",
            "Iteration: 100, loss: 61739.832031\n",
            "Iteration: 125, loss: 58748.843750\n",
            "Iteration: 150, loss: 56555.769531\n",
            "Iteration: 175, loss: 54667.882812\n",
            "Iteration: 200, loss: 53192.613281\n",
            "Iteration: 225, loss: 51979.972656\n",
            "Iteration: 250, loss: 50863.640625\n",
            "Iteration: 275, loss: 49942.875000\n",
            "Iteration: 300, loss: 49076.917969\n",
            "Iteration: 325, loss: 48310.226562\n",
            "Iteration: 350, loss: 47623.968750\n",
            "Iteration: 375, loss: 47032.289062\n",
            "Iteration: 400, loss: 46486.609375\n",
            "Iteration: 425, loss: 45962.253906\n",
            "Iteration: 450, loss: 45505.316406\n",
            "Iteration: 475, loss: 45066.539062\n",
            "Iteration: 500, loss: 44656.339844\n",
            "Iteration: 525, loss: 44280.566406\n",
            "Iteration: 550, loss: 43920.019531\n",
            "Iteration: 575, loss: 43578.523438\n",
            "Iteration: 600, loss: 43251.078125\n",
            "Iteration: 625, loss: 42961.722656\n",
            "Iteration: 650, loss: 42669.730469\n",
            "Iteration: 675, loss: 42414.953125\n",
            "Iteration: 700, loss: 42166.011719\n",
            "Iteration: 725, loss: 41946.214844\n",
            "Iteration: 750, loss: 41737.390625\n",
            "Iteration: 775, loss: 41544.921875\n",
            "Iteration: 800, loss: 41370.539062\n",
            "Iteration: 825, loss: 41203.191406\n",
            "Iteration: 850, loss: 41052.300781\n",
            "Iteration: 875, loss: 40917.468750\n",
            "Iteration: 900, loss: 40795.082031\n",
            "Iteration: 925, loss: 40677.507812\n",
            "Iteration: 950, loss: 40574.546875\n",
            "Iteration: 975, loss: 40467.242188\n",
            "Iteration: 1000, loss: 40372.953125\n",
            "Iteration: 1025, loss: 40283.894531\n",
            "Iteration: 1050, loss: 40202.144531\n",
            "Iteration: 1075, loss: 40124.527344\n",
            "Iteration: 1100, loss: 40056.136719\n",
            "Iteration: 1125, loss: 39986.996094\n",
            "Iteration: 1150, loss: 39926.011719\n",
            "Iteration: 1175, loss: 39865.847656\n",
            "Iteration: 1200, loss: 39806.195312\n",
            "Iteration: 1225, loss: 39751.812500\n",
            "Iteration: 1250, loss: 39705.539062\n",
            "Iteration: 1275, loss: 39661.097656\n",
            "Iteration: 1300, loss: 39619.117188\n",
            "Iteration: 1325, loss: 39574.363281\n",
            "Iteration: 1350, loss: 39538.191406\n",
            "Iteration: 1375, loss: 39505.902344\n",
            "Iteration: 1400, loss: 39476.511719\n",
            "Iteration: 1425, loss: 39445.472656\n",
            "Iteration: 1450, loss: 39416.761719\n",
            "Iteration: 1475, loss: 39389.535156\n",
            "Iteration: 1500, loss: 39361.160156\n",
            "Iteration: 1525, loss: 39333.351562\n",
            "Iteration: 1550, loss: 39306.769531\n",
            "Iteration: 1575, loss: 39282.718750\n",
            "Iteration: 1600, loss: 39260.652344\n",
            "Iteration: 1625, loss: 39239.121094\n",
            "Iteration: 1650, loss: 39216.175781\n",
            "Iteration: 1675, loss: 39197.746094\n",
            "Iteration: 1700, loss: 39176.300781\n",
            "Iteration: 1725, loss: 39158.886719\n",
            "Iteration: 1750, loss: 39140.972656\n",
            "Iteration: 1775, loss: 39125.988281\n",
            "Iteration: 1800, loss: 39109.871094\n",
            "Iteration: 1825, loss: 39093.492188\n",
            "Iteration: 1850, loss: 39076.734375\n",
            "Iteration: 1875, loss: 39063.242188\n",
            "Iteration: 1900, loss: 39049.496094\n",
            "Iteration: 1925, loss: 39036.226562\n",
            "Iteration: 1950, loss: 39023.386719\n",
            "Iteration: 1975, loss: 39010.878906\n",
            "Iteration: 2000, loss: 39000.296875\n",
            "Iteration: 25, loss: 59128.960938\n",
            "Iteration: 50, loss: 46309.289062\n",
            "Iteration: 75, loss: 40493.058594\n",
            "Iteration: 100, loss: 37007.460938\n",
            "Iteration: 125, loss: 34486.855469\n",
            "Iteration: 150, loss: 32440.078125\n",
            "Iteration: 175, loss: 30799.060547\n",
            "Iteration: 200, loss: 29466.066406\n",
            "Iteration: 225, loss: 28303.718750\n",
            "Iteration: 250, loss: 27314.582031\n",
            "Iteration: 275, loss: 26429.826172\n",
            "Iteration: 300, loss: 25656.285156\n",
            "Iteration: 325, loss: 24989.781250\n",
            "Iteration: 350, loss: 24371.128906\n",
            "Iteration: 375, loss: 23895.828125\n",
            "Iteration: 400, loss: 23439.914062\n",
            "Iteration: 425, loss: 23039.605469\n",
            "Iteration: 450, loss: 22691.341797\n",
            "Iteration: 475, loss: 22373.361328\n",
            "Iteration: 500, loss: 22101.636719\n",
            "Iteration: 525, loss: 21848.906250\n",
            "Iteration: 550, loss: 21615.363281\n",
            "Iteration: 575, loss: 21407.845703\n",
            "Iteration: 600, loss: 21217.373047\n",
            "Iteration: 625, loss: 21050.742188\n",
            "Iteration: 650, loss: 20900.033203\n",
            "Iteration: 675, loss: 20762.275391\n",
            "Iteration: 700, loss: 20641.921875\n",
            "Iteration: 725, loss: 20533.404297\n",
            "Iteration: 750, loss: 20425.582031\n",
            "Iteration: 775, loss: 20318.003906\n",
            "Iteration: 800, loss: 20222.511719\n",
            "Iteration: 825, loss: 20140.796875\n",
            "Iteration: 850, loss: 20066.257812\n",
            "Iteration: 875, loss: 19992.410156\n",
            "Iteration: 900, loss: 19925.710938\n",
            "Iteration: 925, loss: 19873.777344\n",
            "Iteration: 950, loss: 19826.304688\n",
            "Iteration: 975, loss: 19774.333984\n",
            "Iteration: 1000, loss: 19730.994141\n",
            "Iteration: 1025, loss: 19693.281250\n",
            "Iteration: 1050, loss: 19657.914062\n",
            "Iteration: 1075, loss: 19625.208984\n",
            "Iteration: 1100, loss: 19597.712891\n",
            "Iteration: 1125, loss: 19570.773438\n",
            "Iteration: 1150, loss: 19543.822266\n",
            "Iteration: 1175, loss: 19515.093750\n",
            "Iteration: 1200, loss: 19492.476562\n",
            "Iteration: 1225, loss: 19472.009766\n",
            "Iteration: 1250, loss: 19451.943359\n",
            "Iteration: 1275, loss: 19431.490234\n",
            "Iteration: 1300, loss: 19413.816406\n",
            "Iteration: 1325, loss: 19398.447266\n",
            "Iteration: 1350, loss: 19382.759766\n",
            "Iteration: 1375, loss: 19365.765625\n",
            "Iteration: 1400, loss: 19352.158203\n",
            "Iteration: 1425, loss: 19340.849609\n",
            "Iteration: 1450, loss: 19329.125000\n",
            "Iteration: 1475, loss: 19317.167969\n",
            "Iteration: 1500, loss: 19305.437500\n",
            "Iteration: 1525, loss: 19296.107422\n",
            "Iteration: 1550, loss: 19287.548828\n",
            "Iteration: 1575, loss: 19278.947266\n",
            "Iteration: 1600, loss: 19270.357422\n",
            "Iteration: 1625, loss: 19263.113281\n",
            "Iteration: 1650, loss: 19255.648438\n",
            "Iteration: 1675, loss: 19248.496094\n",
            "Iteration: 1700, loss: 19240.988281\n",
            "Iteration: 1725, loss: 19234.941406\n",
            "Iteration: 1750, loss: 19228.962891\n",
            "Iteration: 1775, loss: 19223.224609\n",
            "Iteration: 1800, loss: 19216.951172\n",
            "Iteration: 1825, loss: 19211.251953\n",
            "Iteration: 1850, loss: 19206.539062\n",
            "Iteration: 1875, loss: 19201.433594\n",
            "Iteration: 1900, loss: 19196.369141\n",
            "Iteration: 1925, loss: 19191.650391\n",
            "Iteration: 1950, loss: 19187.693359\n",
            "Iteration: 1975, loss: 19183.414062\n",
            "Iteration: 2000, loss: 19178.662109\n",
            "Iteration: 25, loss: 243187.421875\n",
            "Iteration: 50, loss: 181567.671875\n",
            "Iteration: 75, loss: 163813.015625\n",
            "Iteration: 100, loss: 153743.640625\n",
            "Iteration: 125, loss: 146521.468750\n",
            "Iteration: 150, loss: 140752.250000\n",
            "Iteration: 175, loss: 136585.859375\n",
            "Iteration: 200, loss: 133200.984375\n",
            "Iteration: 225, loss: 130127.210938\n",
            "Iteration: 250, loss: 127708.382812\n",
            "Iteration: 275, loss: 125524.414062\n",
            "Iteration: 300, loss: 123544.828125\n",
            "Iteration: 325, loss: 121770.273438\n",
            "Iteration: 350, loss: 119976.750000\n",
            "Iteration: 375, loss: 118410.765625\n",
            "Iteration: 400, loss: 117018.039062\n",
            "Iteration: 425, loss: 115634.429688\n",
            "Iteration: 450, loss: 114301.203125\n",
            "Iteration: 475, loss: 113002.664062\n",
            "Iteration: 500, loss: 111860.179688\n",
            "Iteration: 525, loss: 110769.359375\n",
            "Iteration: 550, loss: 109723.414062\n",
            "Iteration: 575, loss: 108755.343750\n",
            "Iteration: 600, loss: 107776.125000\n",
            "Iteration: 625, loss: 106862.117188\n",
            "Iteration: 650, loss: 106021.046875\n",
            "Iteration: 675, loss: 105210.687500\n",
            "Iteration: 700, loss: 104458.742188\n",
            "Iteration: 725, loss: 103715.976562\n",
            "Iteration: 750, loss: 102948.296875\n",
            "Iteration: 775, loss: 102229.062500\n",
            "Iteration: 800, loss: 101520.101562\n",
            "Iteration: 825, loss: 100754.960938\n",
            "Iteration: 850, loss: 99999.609375\n",
            "Iteration: 875, loss: 99295.078125\n",
            "Iteration: 900, loss: 98669.953125\n",
            "Iteration: 925, loss: 98082.226562\n",
            "Iteration: 950, loss: 97503.562500\n",
            "Iteration: 975, loss: 96917.031250\n",
            "Iteration: 1000, loss: 96407.382812\n",
            "Iteration: 1025, loss: 95869.164062\n",
            "Iteration: 1050, loss: 95395.921875\n",
            "Iteration: 1075, loss: 94905.890625\n",
            "Iteration: 1100, loss: 94396.625000\n",
            "Iteration: 1125, loss: 93954.890625\n",
            "Iteration: 1150, loss: 93516.757812\n",
            "Iteration: 1175, loss: 93110.953125\n",
            "Iteration: 1200, loss: 92718.656250\n",
            "Iteration: 1225, loss: 92312.335938\n",
            "Iteration: 1250, loss: 91937.554688\n",
            "Iteration: 1275, loss: 91567.937500\n",
            "Iteration: 1300, loss: 91199.765625\n",
            "Iteration: 1325, loss: 90841.828125\n",
            "Iteration: 1350, loss: 90473.984375\n",
            "Iteration: 1375, loss: 90120.421875\n",
            "Iteration: 1400, loss: 89770.992188\n",
            "Iteration: 1425, loss: 89485.734375\n",
            "Iteration: 1450, loss: 89181.984375\n",
            "Iteration: 1475, loss: 88881.398438\n",
            "Iteration: 1500, loss: 88575.046875\n",
            "Iteration: 1525, loss: 88268.656250\n",
            "Iteration: 1550, loss: 87964.687500\n",
            "Iteration: 1575, loss: 87662.562500\n",
            "Iteration: 1600, loss: 87374.828125\n",
            "Iteration: 1625, loss: 87067.750000\n",
            "Iteration: 1650, loss: 86792.945312\n",
            "Iteration: 1675, loss: 86522.601562\n",
            "Iteration: 1700, loss: 86240.937500\n",
            "Iteration: 1725, loss: 85966.929688\n",
            "Iteration: 1750, loss: 85696.882812\n",
            "Iteration: 1775, loss: 85439.859375\n",
            "Iteration: 1800, loss: 85170.085938\n",
            "Iteration: 1825, loss: 84925.632812\n",
            "Iteration: 1850, loss: 84680.585938\n",
            "Iteration: 1875, loss: 84465.703125\n",
            "Iteration: 1900, loss: 84241.226562\n",
            "Iteration: 1925, loss: 84024.507812\n",
            "Iteration: 1950, loss: 83810.578125\n",
            "Iteration: 1975, loss: 83617.406250\n",
            "Iteration: 2000, loss: 83441.851562\n",
            "Iteration: 25, loss: 321608.125000\n",
            "Iteration: 50, loss: 166990.343750\n",
            "Iteration: 75, loss: 140440.328125\n",
            "Iteration: 100, loss: 122263.804688\n",
            "Iteration: 125, loss: 119396.218750\n",
            "Iteration: 150, loss: 110032.648438\n",
            "Iteration: 175, loss: 105866.515625\n",
            "Iteration: 200, loss: 103033.187500\n",
            "Iteration: 225, loss: 100119.375000\n",
            "Iteration: 250, loss: 97218.625000\n",
            "Iteration: 275, loss: 95363.484375\n",
            "Iteration: 300, loss: 93677.250000\n",
            "Iteration: 325, loss: 92077.335938\n",
            "Iteration: 350, loss: 90397.554688\n",
            "Iteration: 375, loss: 88823.289062\n",
            "Iteration: 400, loss: 87512.359375\n",
            "Iteration: 425, loss: 86369.781250\n",
            "Iteration: 450, loss: 85336.148438\n",
            "Iteration: 475, loss: 84263.429688\n",
            "Iteration: 500, loss: 83288.921875\n",
            "Iteration: 525, loss: 82313.679688\n",
            "Iteration: 550, loss: 81475.757812\n",
            "Iteration: 575, loss: 80629.804688\n",
            "Iteration: 600, loss: 79935.023438\n",
            "Iteration: 625, loss: 79228.078125\n",
            "Iteration: 650, loss: 78527.671875\n",
            "Iteration: 675, loss: 77898.187500\n",
            "Iteration: 700, loss: 77258.960938\n",
            "Iteration: 725, loss: 76684.437500\n",
            "Iteration: 750, loss: 76140.265625\n",
            "Iteration: 775, loss: 75618.312500\n",
            "Iteration: 800, loss: 75149.570312\n",
            "Iteration: 825, loss: 74701.257812\n",
            "Iteration: 850, loss: 74274.242188\n",
            "Iteration: 875, loss: 73899.609375\n",
            "Iteration: 900, loss: 73533.585938\n",
            "Iteration: 925, loss: 73171.570312\n",
            "Iteration: 950, loss: 72833.132812\n",
            "Iteration: 975, loss: 72503.945312\n",
            "Iteration: 1000, loss: 72180.562500\n",
            "Iteration: 1025, loss: 71856.000000\n",
            "Iteration: 1050, loss: 71571.937500\n",
            "Iteration: 1075, loss: 71288.179688\n",
            "Iteration: 1100, loss: 71028.117188\n",
            "Iteration: 1125, loss: 70770.156250\n",
            "Iteration: 1150, loss: 70518.453125\n",
            "Iteration: 1175, loss: 70290.953125\n",
            "Iteration: 1200, loss: 70054.507812\n",
            "Iteration: 1225, loss: 69827.476562\n",
            "Iteration: 1250, loss: 69602.601562\n",
            "Iteration: 1275, loss: 69400.148438\n",
            "Iteration: 1300, loss: 69212.492188\n",
            "Iteration: 1325, loss: 69016.250000\n",
            "Iteration: 1350, loss: 68814.164062\n",
            "Iteration: 1375, loss: 68647.304688\n",
            "Iteration: 1400, loss: 68497.062500\n",
            "Iteration: 1425, loss: 68380.898438\n",
            "Iteration: 1450, loss: 68185.703125\n",
            "Iteration: 1475, loss: 68028.148438\n",
            "Iteration: 1500, loss: 67881.546875\n",
            "Iteration: 1525, loss: 67745.445312\n",
            "Iteration: 1550, loss: 67573.609375\n",
            "Iteration: 1575, loss: 67408.945312\n",
            "Iteration: 1600, loss: 67276.687500\n",
            "Iteration: 1625, loss: 67135.742188\n",
            "Iteration: 1650, loss: 66980.781250\n",
            "Iteration: 1675, loss: 66827.742188\n",
            "Iteration: 1700, loss: 66745.648438\n",
            "Iteration: 1725, loss: 66538.679688\n",
            "Iteration: 1750, loss: 66401.500000\n",
            "Iteration: 1775, loss: 66263.070312\n",
            "Iteration: 1800, loss: 66119.937500\n",
            "Iteration: 1825, loss: 66006.617188\n",
            "Iteration: 1850, loss: 65868.617188\n",
            "Iteration: 1875, loss: 65738.195312\n",
            "Iteration: 1900, loss: 65631.203125\n",
            "Iteration: 1925, loss: 65527.828125\n",
            "Iteration: 1950, loss: 65419.058594\n",
            "Iteration: 1975, loss: 65314.863281\n",
            "Iteration: 2000, loss: 65201.167969\n",
            "Iteration: 25, loss: 144121.125000\n",
            "Iteration: 50, loss: 117681.906250\n",
            "Iteration: 75, loss: 106247.156250\n",
            "Iteration: 100, loss: 100605.984375\n",
            "Iteration: 125, loss: 96588.453125\n",
            "Iteration: 150, loss: 93148.156250\n",
            "Iteration: 175, loss: 90240.906250\n",
            "Iteration: 200, loss: 88025.562500\n",
            "Iteration: 225, loss: 85993.132812\n",
            "Iteration: 250, loss: 84340.539062\n",
            "Iteration: 275, loss: 82833.328125\n",
            "Iteration: 300, loss: 81429.976562\n",
            "Iteration: 325, loss: 80181.671875\n",
            "Iteration: 350, loss: 79011.570312\n",
            "Iteration: 375, loss: 77859.531250\n",
            "Iteration: 400, loss: 76822.195312\n",
            "Iteration: 425, loss: 75836.117188\n",
            "Iteration: 450, loss: 74987.226562\n",
            "Iteration: 475, loss: 74073.757812\n",
            "Iteration: 500, loss: 73301.695312\n",
            "Iteration: 525, loss: 72518.195312\n",
            "Iteration: 550, loss: 71784.101562\n",
            "Iteration: 575, loss: 71091.757812\n",
            "Iteration: 600, loss: 70411.718750\n",
            "Iteration: 625, loss: 69788.968750\n",
            "Iteration: 650, loss: 69138.898438\n",
            "Iteration: 675, loss: 68553.570312\n",
            "Iteration: 700, loss: 67997.210938\n",
            "Iteration: 725, loss: 67430.351562\n",
            "Iteration: 750, loss: 66952.101562\n",
            "Iteration: 775, loss: 66476.734375\n",
            "Iteration: 800, loss: 66029.984375\n",
            "Iteration: 825, loss: 65592.898438\n",
            "Iteration: 850, loss: 65153.156250\n",
            "Iteration: 875, loss: 64753.128906\n",
            "Iteration: 900, loss: 64350.101562\n",
            "Iteration: 925, loss: 63989.468750\n",
            "Iteration: 950, loss: 63617.187500\n",
            "Iteration: 975, loss: 63302.578125\n",
            "Iteration: 1000, loss: 62962.652344\n",
            "Iteration: 1025, loss: 62657.039062\n",
            "Iteration: 1050, loss: 62349.894531\n",
            "Iteration: 1075, loss: 62057.207031\n",
            "Iteration: 1100, loss: 61753.113281\n",
            "Iteration: 1125, loss: 61435.003906\n",
            "Iteration: 1150, loss: 61170.781250\n",
            "Iteration: 1175, loss: 60898.187500\n",
            "Iteration: 1200, loss: 60625.898438\n",
            "Iteration: 1225, loss: 60375.500000\n",
            "Iteration: 1250, loss: 60139.976562\n",
            "Iteration: 1275, loss: 59905.769531\n",
            "Iteration: 1300, loss: 59680.859375\n",
            "Iteration: 1325, loss: 59455.667969\n",
            "Iteration: 1350, loss: 59248.148438\n",
            "Iteration: 1375, loss: 59040.972656\n",
            "Iteration: 1400, loss: 58846.441406\n",
            "Iteration: 1425, loss: 58641.585938\n",
            "Iteration: 1450, loss: 58441.113281\n",
            "Iteration: 1475, loss: 58242.113281\n",
            "Iteration: 1500, loss: 58075.234375\n",
            "Iteration: 1525, loss: 57877.230469\n",
            "Iteration: 1550, loss: 57711.843750\n",
            "Iteration: 1575, loss: 57532.816406\n",
            "Iteration: 1600, loss: 57365.542969\n",
            "Iteration: 1625, loss: 57194.285156\n",
            "Iteration: 1650, loss: 57046.910156\n",
            "Iteration: 1675, loss: 56879.234375\n",
            "Iteration: 1700, loss: 56713.000000\n",
            "Iteration: 1725, loss: 56561.906250\n",
            "Iteration: 1750, loss: 56403.328125\n",
            "Iteration: 1775, loss: 56268.644531\n",
            "Iteration: 1800, loss: 56116.894531\n",
            "Iteration: 1825, loss: 55967.050781\n",
            "Iteration: 1850, loss: 55809.007812\n",
            "Iteration: 1875, loss: 55666.339844\n",
            "Iteration: 1900, loss: 55512.515625\n",
            "Iteration: 1925, loss: 55369.992188\n",
            "Iteration: 1950, loss: 55227.878906\n",
            "Iteration: 1975, loss: 55107.687500\n",
            "Iteration: 2000, loss: 54999.582031\n",
            "Iteration: 25, loss: 86002.890625\n",
            "Iteration: 50, loss: 69342.312500\n",
            "Iteration: 75, loss: 62367.617188\n",
            "Iteration: 100, loss: 58604.074219\n",
            "Iteration: 125, loss: 56057.152344\n",
            "Iteration: 150, loss: 54308.988281\n",
            "Iteration: 175, loss: 52872.003906\n",
            "Iteration: 200, loss: 51729.558594\n",
            "Iteration: 225, loss: 50789.457031\n",
            "Iteration: 250, loss: 49963.261719\n",
            "Iteration: 275, loss: 49272.792969\n",
            "Iteration: 300, loss: 48687.539062\n",
            "Iteration: 325, loss: 48190.203125\n",
            "Iteration: 350, loss: 47748.593750\n",
            "Iteration: 375, loss: 47373.085938\n",
            "Iteration: 400, loss: 47049.281250\n",
            "Iteration: 425, loss: 46763.585938\n",
            "Iteration: 450, loss: 46501.003906\n",
            "Iteration: 475, loss: 46284.511719\n",
            "Iteration: 500, loss: 46109.031250\n",
            "Iteration: 525, loss: 45929.718750\n",
            "Iteration: 550, loss: 45782.601562\n",
            "Iteration: 575, loss: 45651.007812\n",
            "Iteration: 600, loss: 45528.257812\n",
            "Iteration: 625, loss: 45421.144531\n",
            "Iteration: 650, loss: 45325.167969\n",
            "Iteration: 675, loss: 45242.550781\n",
            "Iteration: 700, loss: 45164.097656\n",
            "Iteration: 725, loss: 45093.835938\n",
            "Iteration: 750, loss: 45031.398438\n",
            "Iteration: 775, loss: 44974.292969\n",
            "Iteration: 800, loss: 44927.484375\n",
            "Iteration: 825, loss: 44880.562500\n",
            "Iteration: 850, loss: 44836.648438\n",
            "Iteration: 875, loss: 44796.207031\n",
            "Iteration: 900, loss: 44761.847656\n",
            "Iteration: 925, loss: 44727.667969\n",
            "Iteration: 950, loss: 44693.027344\n",
            "Iteration: 975, loss: 44661.433594\n",
            "Iteration: 1000, loss: 44632.675781\n",
            "Iteration: 1025, loss: 44607.644531\n",
            "Iteration: 1050, loss: 44582.378906\n",
            "Iteration: 1075, loss: 44557.671875\n",
            "Iteration: 1100, loss: 44536.753906\n",
            "Iteration: 1125, loss: 44518.746094\n",
            "Iteration: 1150, loss: 44500.558594\n",
            "Iteration: 1175, loss: 44482.660156\n",
            "Iteration: 1200, loss: 44466.042969\n",
            "Iteration: 1225, loss: 44451.777344\n",
            "Iteration: 1250, loss: 44438.113281\n",
            "Iteration: 1275, loss: 44423.914062\n",
            "Iteration: 1300, loss: 44410.562500\n",
            "Iteration: 1325, loss: 44398.320312\n",
            "Iteration: 1350, loss: 44387.171875\n",
            "Iteration: 1375, loss: 44375.730469\n",
            "Iteration: 1400, loss: 44366.226562\n",
            "Iteration: 1425, loss: 44355.757812\n",
            "Iteration: 1450, loss: 44346.671875\n",
            "Iteration: 1475, loss: 44337.406250\n",
            "Iteration: 1500, loss: 44329.320312\n",
            "Iteration: 1525, loss: 44321.117188\n",
            "Iteration: 1550, loss: 44313.398438\n",
            "Iteration: 1575, loss: 44305.792969\n",
            "Iteration: 1600, loss: 44298.871094\n",
            "Iteration: 1625, loss: 44291.945312\n",
            "Iteration: 1650, loss: 44285.468750\n",
            "Iteration: 1675, loss: 44278.660156\n",
            "Iteration: 1700, loss: 44272.671875\n",
            "Iteration: 1725, loss: 44266.898438\n",
            "Iteration: 1750, loss: 44261.292969\n",
            "Iteration: 1775, loss: 44256.429688\n",
            "Iteration: 1800, loss: 44251.140625\n",
            "Iteration: 1825, loss: 44246.226562\n",
            "Iteration: 1850, loss: 44241.742188\n",
            "Iteration: 1875, loss: 44237.664062\n",
            "Iteration: 1900, loss: 44233.320312\n",
            "Iteration: 1925, loss: 44229.503906\n",
            "Iteration: 1950, loss: 44225.660156\n",
            "Iteration: 1975, loss: 44222.437500\n",
            "Iteration: 2000, loss: 44218.832031\n"
          ]
        }
      ],
      "source": [
        "style_losses = []\n",
        "for i in range(len(style_images)):\n",
        "    style_losses.append(run_transfer(style_images[i], content_images[i], \"style\", 2000, 25, output_dir+'style_avg_2000/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "content_losses_np = np.array(content_losses)\n",
        "random_losses_np = np.array(random_losses)\n",
        "style_losses_np = np.array(style_losses)\n",
        "\n",
        "np.savetxt(output_dir+\"content_avg_losses.csv\", content_losses_np, delimiter=\",\")\n",
        "np.savetxt(output_dir+\"random_avg_losses.csv\", random_losses_np, delimiter=\",\")\n",
        "np.savetxt(output_dir+\"style_avg_losses.csv\", style_losses_np, delimiter=\",\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_losses_np = np.genfromtxt(output_dir+\"content_avg_losses.csv\", delimiter=\",\")\n",
        "random_losses_np = np.genfromtxt(output_dir+\"random_avg_losses.csv\", delimiter=\",\")\n",
        "style_losses_np = np.genfromtxt(output_dir+\"style_avg_losses.csv\", delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=[10,6])\n",
        "plt.plot(range(0, 2000, 25), np.log(content_losses_np.mean(axis=0)), color = \"blue\")\n",
        "plt.plot(range(0, 2000, 25), np.log(random_losses_np.mean(axis=0)), color = \"orange\")\n",
        "plt.plot(range(0, 2000, 25), np.log(style_losses_np.mean(axis=0)), color = \"red\")\n",
        "plt.legend(labels = [\"Content\", \"Random\", \"Style\"], loc = \"upper right\")\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"average total loss\")\n",
        "plt.title(\"Log Average Total Loss vs. Number of Epochs (Avg Pooling)\")\n",
        "plt.draw()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 287420.718750\n",
            "Iteration: 50, loss: 109534.992188\n",
            "Iteration: 75, loss: 73757.781250\n",
            "Iteration: 100, loss: 61850.632812\n",
            "Iteration: 125, loss: 55850.664062\n",
            "Iteration: 150, loss: 52117.757812\n",
            "Iteration: 175, loss: 49624.195312\n",
            "Iteration: 200, loss: 47836.562500\n",
            "Iteration: 225, loss: 46396.468750\n",
            "Iteration: 250, loss: 45286.234375\n",
            "Iteration: 275, loss: 44418.609375\n",
            "Iteration: 300, loss: 43742.800781\n",
            "Iteration: 325, loss: 43133.242188\n",
            "Iteration: 350, loss: 42621.312500\n",
            "Iteration: 375, loss: 42179.359375\n",
            "Iteration: 400, loss: 41817.390625\n",
            "Iteration: 425, loss: 41503.046875\n",
            "Iteration: 450, loss: 41222.148438\n",
            "Iteration: 475, loss: 40966.671875\n",
            "Iteration: 500, loss: 40728.714844\n",
            "Iteration: 525, loss: 40511.933594\n",
            "Iteration: 550, loss: 40313.296875\n",
            "Iteration: 575, loss: 40140.765625\n",
            "Iteration: 600, loss: 39994.964844\n",
            "Iteration: 625, loss: 39856.945312\n",
            "Iteration: 650, loss: 39729.507812\n",
            "Iteration: 675, loss: 39617.945312\n",
            "Iteration: 700, loss: 39502.437500\n",
            "Iteration: 725, loss: 39410.906250\n",
            "Iteration: 750, loss: 39326.777344\n",
            "Iteration: 775, loss: 39249.433594\n",
            "Iteration: 800, loss: 39170.453125\n",
            "Iteration: 825, loss: 39100.394531\n",
            "Iteration: 850, loss: 39043.136719\n",
            "Iteration: 875, loss: 38985.199219\n",
            "Iteration: 900, loss: 38933.765625\n",
            "Iteration: 925, loss: 38882.648438\n",
            "Iteration: 950, loss: 38838.601562\n",
            "Iteration: 975, loss: 38799.730469\n",
            "Iteration: 1000, loss: 38765.542969\n",
            "Iteration: 25, loss: 644654.000000\n",
            "Iteration: 50, loss: 171232.734375\n",
            "Iteration: 75, loss: 69747.507812\n",
            "Iteration: 100, loss: 44891.132812\n",
            "Iteration: 125, loss: 35660.796875\n",
            "Iteration: 150, loss: 30888.277344\n",
            "Iteration: 175, loss: 28231.968750\n",
            "Iteration: 200, loss: 26340.916016\n",
            "Iteration: 225, loss: 24982.822266\n",
            "Iteration: 250, loss: 23940.230469\n",
            "Iteration: 275, loss: 23120.277344\n",
            "Iteration: 300, loss: 22480.095703\n",
            "Iteration: 325, loss: 21938.470703\n",
            "Iteration: 350, loss: 21506.611328\n",
            "Iteration: 375, loss: 21183.824219\n",
            "Iteration: 400, loss: 20893.796875\n",
            "Iteration: 425, loss: 20660.667969\n",
            "Iteration: 450, loss: 20475.783203\n",
            "Iteration: 475, loss: 20311.800781\n",
            "Iteration: 500, loss: 20171.855469\n",
            "Iteration: 525, loss: 20056.414062\n",
            "Iteration: 550, loss: 19952.445312\n",
            "Iteration: 575, loss: 19869.423828\n",
            "Iteration: 600, loss: 19789.767578\n",
            "Iteration: 625, loss: 19724.628906\n",
            "Iteration: 650, loss: 19669.861328\n",
            "Iteration: 675, loss: 19621.193359\n",
            "Iteration: 700, loss: 19576.068359\n",
            "Iteration: 725, loss: 19538.308594\n",
            "Iteration: 750, loss: 19505.132812\n",
            "Iteration: 775, loss: 19476.523438\n",
            "Iteration: 800, loss: 19448.550781\n",
            "Iteration: 825, loss: 19424.660156\n",
            "Iteration: 850, loss: 19404.283203\n",
            "Iteration: 875, loss: 19386.585938\n",
            "Iteration: 900, loss: 19367.781250\n",
            "Iteration: 925, loss: 19350.554688\n",
            "Iteration: 950, loss: 19336.320312\n",
            "Iteration: 975, loss: 19323.015625\n",
            "Iteration: 1000, loss: 19309.099609\n",
            "Iteration: 25, loss: 1546573.000000\n",
            "Iteration: 50, loss: 457676.375000\n",
            "Iteration: 75, loss: 252234.875000\n",
            "Iteration: 100, loss: 185084.375000\n",
            "Iteration: 125, loss: 149769.156250\n",
            "Iteration: 150, loss: 130425.828125\n",
            "Iteration: 175, loss: 118804.640625\n",
            "Iteration: 200, loss: 111055.484375\n",
            "Iteration: 225, loss: 105158.906250\n",
            "Iteration: 250, loss: 100960.281250\n",
            "Iteration: 275, loss: 97515.921875\n",
            "Iteration: 300, loss: 94833.164062\n",
            "Iteration: 325, loss: 92448.796875\n",
            "Iteration: 350, loss: 90494.710938\n",
            "Iteration: 375, loss: 88790.281250\n",
            "Iteration: 400, loss: 87370.109375\n",
            "Iteration: 425, loss: 86095.593750\n",
            "Iteration: 450, loss: 85020.281250\n",
            "Iteration: 475, loss: 84026.984375\n",
            "Iteration: 500, loss: 83072.671875\n",
            "Iteration: 525, loss: 82238.226562\n",
            "Iteration: 550, loss: 81498.593750\n",
            "Iteration: 575, loss: 80823.828125\n",
            "Iteration: 600, loss: 80262.421875\n",
            "Iteration: 625, loss: 79733.906250\n",
            "Iteration: 650, loss: 79226.734375\n",
            "Iteration: 675, loss: 78813.203125\n",
            "Iteration: 700, loss: 78411.093750\n",
            "Iteration: 725, loss: 78093.578125\n",
            "Iteration: 750, loss: 77782.156250\n",
            "Iteration: 775, loss: 77489.656250\n",
            "Iteration: 800, loss: 77231.820312\n",
            "Iteration: 825, loss: 76978.945312\n",
            "Iteration: 850, loss: 76766.984375\n",
            "Iteration: 875, loss: 76572.039062\n",
            "Iteration: 900, loss: 76385.460938\n",
            "Iteration: 925, loss: 76221.367188\n",
            "Iteration: 950, loss: 76077.093750\n",
            "Iteration: 975, loss: 75928.617188\n",
            "Iteration: 1000, loss: 75797.492188\n",
            "Iteration: 25, loss: 3410906.250000\n",
            "Iteration: 50, loss: 815527.500000\n",
            "Iteration: 75, loss: 351919.250000\n",
            "Iteration: 100, loss: 226268.609375\n",
            "Iteration: 125, loss: 177421.968750\n",
            "Iteration: 150, loss: 143334.281250\n",
            "Iteration: 175, loss: 121490.664062\n",
            "Iteration: 200, loss: 110664.296875\n",
            "Iteration: 225, loss: 99846.046875\n",
            "Iteration: 250, loss: 93868.804688\n",
            "Iteration: 275, loss: 88300.687500\n",
            "Iteration: 300, loss: 84202.718750\n",
            "Iteration: 325, loss: 80763.875000\n",
            "Iteration: 350, loss: 78082.976562\n",
            "Iteration: 375, loss: 75778.406250\n",
            "Iteration: 400, loss: 73552.039062\n",
            "Iteration: 425, loss: 71844.187500\n",
            "Iteration: 450, loss: 70433.632812\n",
            "Iteration: 475, loss: 69210.078125\n",
            "Iteration: 500, loss: 68180.156250\n",
            "Iteration: 525, loss: 67232.273438\n",
            "Iteration: 550, loss: 66324.578125\n",
            "Iteration: 575, loss: 65527.226562\n",
            "Iteration: 600, loss: 64862.890625\n",
            "Iteration: 625, loss: 64225.609375\n",
            "Iteration: 650, loss: 63679.609375\n",
            "Iteration: 675, loss: 63162.085938\n",
            "Iteration: 700, loss: 62656.140625\n",
            "Iteration: 725, loss: 62188.937500\n",
            "Iteration: 750, loss: 61786.984375\n",
            "Iteration: 775, loss: 61376.781250\n",
            "Iteration: 800, loss: 61052.750000\n",
            "Iteration: 825, loss: 60695.156250\n",
            "Iteration: 850, loss: 60359.140625\n",
            "Iteration: 875, loss: 60060.097656\n",
            "Iteration: 900, loss: 59776.363281\n",
            "Iteration: 925, loss: 59533.160156\n",
            "Iteration: 950, loss: 59301.343750\n",
            "Iteration: 975, loss: 59084.242188\n",
            "Iteration: 1000, loss: 58895.632812\n",
            "Iteration: 25, loss: 628122.375000\n",
            "Iteration: 50, loss: 277251.937500\n",
            "Iteration: 75, loss: 175326.421875\n",
            "Iteration: 100, loss: 127444.250000\n",
            "Iteration: 125, loss: 104702.812500\n",
            "Iteration: 150, loss: 90194.593750\n",
            "Iteration: 175, loss: 82019.765625\n",
            "Iteration: 200, loss: 76067.328125\n",
            "Iteration: 225, loss: 72010.468750\n",
            "Iteration: 250, loss: 69013.125000\n",
            "Iteration: 275, loss: 66764.312500\n",
            "Iteration: 300, loss: 64742.898438\n",
            "Iteration: 325, loss: 63063.144531\n",
            "Iteration: 350, loss: 61585.574219\n",
            "Iteration: 375, loss: 60316.210938\n",
            "Iteration: 400, loss: 59223.632812\n",
            "Iteration: 425, loss: 58251.714844\n",
            "Iteration: 450, loss: 57405.601562\n",
            "Iteration: 475, loss: 56623.082031\n",
            "Iteration: 500, loss: 55917.867188\n",
            "Iteration: 525, loss: 55319.449219\n",
            "Iteration: 550, loss: 54758.761719\n",
            "Iteration: 575, loss: 54279.234375\n",
            "Iteration: 600, loss: 53858.031250\n",
            "Iteration: 625, loss: 53471.546875\n",
            "Iteration: 650, loss: 53125.601562\n",
            "Iteration: 675, loss: 52791.605469\n",
            "Iteration: 700, loss: 52499.632812\n",
            "Iteration: 725, loss: 52223.710938\n",
            "Iteration: 750, loss: 51980.667969\n",
            "Iteration: 775, loss: 51756.484375\n",
            "Iteration: 800, loss: 51557.125000\n",
            "Iteration: 825, loss: 51372.742188\n",
            "Iteration: 850, loss: 51194.226562\n",
            "Iteration: 875, loss: 51024.156250\n",
            "Iteration: 900, loss: 50884.972656\n",
            "Iteration: 925, loss: 50751.277344\n",
            "Iteration: 950, loss: 50631.121094\n",
            "Iteration: 975, loss: 50506.484375\n",
            "Iteration: 1000, loss: 50411.546875\n",
            "Iteration: 25, loss: 710796.750000\n",
            "Iteration: 50, loss: 232130.703125\n",
            "Iteration: 75, loss: 96005.210938\n",
            "Iteration: 100, loss: 74487.343750\n",
            "Iteration: 125, loss: 65672.179688\n",
            "Iteration: 150, loss: 61051.578125\n",
            "Iteration: 175, loss: 58030.046875\n",
            "Iteration: 200, loss: 55753.628906\n",
            "Iteration: 225, loss: 53915.960938\n",
            "Iteration: 250, loss: 52445.687500\n",
            "Iteration: 275, loss: 51216.660156\n",
            "Iteration: 300, loss: 50197.648438\n",
            "Iteration: 325, loss: 49350.921875\n",
            "Iteration: 350, loss: 48676.984375\n",
            "Iteration: 375, loss: 48075.902344\n",
            "Iteration: 400, loss: 47597.257812\n",
            "Iteration: 425, loss: 47192.195312\n",
            "Iteration: 450, loss: 46846.910156\n",
            "Iteration: 475, loss: 46554.105469\n",
            "Iteration: 500, loss: 46300.484375\n",
            "Iteration: 525, loss: 46079.066406\n",
            "Iteration: 550, loss: 45882.691406\n",
            "Iteration: 575, loss: 45714.250000\n",
            "Iteration: 600, loss: 45571.449219\n",
            "Iteration: 625, loss: 45439.078125\n",
            "Iteration: 650, loss: 45324.562500\n",
            "Iteration: 675, loss: 45221.277344\n",
            "Iteration: 700, loss: 45126.140625\n",
            "Iteration: 725, loss: 45042.351562\n",
            "Iteration: 750, loss: 44960.660156\n",
            "Iteration: 775, loss: 44890.937500\n",
            "Iteration: 800, loss: 44829.265625\n",
            "Iteration: 825, loss: 44775.023438\n",
            "Iteration: 850, loss: 44727.367188\n",
            "Iteration: 875, loss: 44681.195312\n",
            "Iteration: 900, loss: 44641.519531\n",
            "Iteration: 925, loss: 44606.261719\n",
            "Iteration: 950, loss: 44573.070312\n",
            "Iteration: 975, loss: 44542.171875\n",
            "Iteration: 1000, loss: 44514.562500\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"content\", 1000, 25, output_dir+'content_avg_1000/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 1605298.000000\n",
            "Iteration: 50, loss: 308585.375000\n",
            "Iteration: 75, loss: 186435.828125\n",
            "Iteration: 100, loss: 134918.453125\n",
            "Iteration: 125, loss: 108977.539062\n",
            "Iteration: 150, loss: 92382.718750\n",
            "Iteration: 175, loss: 80099.101562\n",
            "Iteration: 200, loss: 71547.742188\n",
            "Iteration: 225, loss: 65431.570312\n",
            "Iteration: 250, loss: 61087.664062\n",
            "Iteration: 275, loss: 57729.367188\n",
            "Iteration: 300, loss: 55254.347656\n",
            "Iteration: 325, loss: 53385.457031\n",
            "Iteration: 350, loss: 51786.632812\n",
            "Iteration: 375, loss: 50488.640625\n",
            "Iteration: 400, loss: 49319.738281\n",
            "Iteration: 425, loss: 48345.503906\n",
            "Iteration: 450, loss: 47531.757812\n",
            "Iteration: 475, loss: 46733.917969\n",
            "Iteration: 500, loss: 46071.339844\n",
            "Iteration: 525, loss: 45432.238281\n",
            "Iteration: 550, loss: 44882.777344\n",
            "Iteration: 575, loss: 44404.265625\n",
            "Iteration: 600, loss: 43933.042969\n",
            "Iteration: 625, loss: 43507.972656\n",
            "Iteration: 650, loss: 43130.812500\n",
            "Iteration: 675, loss: 42777.445312\n",
            "Iteration: 700, loss: 42476.429688\n",
            "Iteration: 725, loss: 42197.710938\n",
            "Iteration: 750, loss: 41927.039062\n",
            "Iteration: 775, loss: 41684.910156\n",
            "Iteration: 800, loss: 41452.500000\n",
            "Iteration: 825, loss: 41247.023438\n",
            "Iteration: 850, loss: 41059.609375\n",
            "Iteration: 875, loss: 40882.660156\n",
            "Iteration: 900, loss: 40722.382812\n",
            "Iteration: 925, loss: 40577.585938\n",
            "Iteration: 950, loss: 40444.511719\n",
            "Iteration: 975, loss: 40323.843750\n",
            "Iteration: 1000, loss: 40211.867188\n",
            "Iteration: 25, loss: 12825138.000000\n",
            "Iteration: 50, loss: 523060.250000\n",
            "Iteration: 75, loss: 160917.546875\n",
            "Iteration: 100, loss: 76424.890625\n",
            "Iteration: 125, loss: 55602.597656\n",
            "Iteration: 150, loss: 45647.156250\n",
            "Iteration: 175, loss: 39819.480469\n",
            "Iteration: 200, loss: 35760.625000\n",
            "Iteration: 225, loss: 33008.734375\n",
            "Iteration: 250, loss: 30799.574219\n",
            "Iteration: 275, loss: 29195.119141\n",
            "Iteration: 300, loss: 27839.554688\n",
            "Iteration: 325, loss: 26766.531250\n",
            "Iteration: 350, loss: 25836.300781\n",
            "Iteration: 375, loss: 25044.781250\n",
            "Iteration: 400, loss: 24412.505859\n",
            "Iteration: 425, loss: 23817.082031\n",
            "Iteration: 450, loss: 23335.025391\n",
            "Iteration: 475, loss: 22917.414062\n",
            "Iteration: 500, loss: 22540.488281\n",
            "Iteration: 525, loss: 22224.378906\n",
            "Iteration: 550, loss: 21936.947266\n",
            "Iteration: 575, loss: 21692.281250\n",
            "Iteration: 600, loss: 21475.396484\n",
            "Iteration: 625, loss: 21288.728516\n",
            "Iteration: 650, loss: 21120.759766\n",
            "Iteration: 675, loss: 20973.152344\n",
            "Iteration: 700, loss: 20839.148438\n",
            "Iteration: 725, loss: 20721.054688\n",
            "Iteration: 750, loss: 20607.371094\n",
            "Iteration: 775, loss: 20517.548828\n",
            "Iteration: 800, loss: 20432.035156\n",
            "Iteration: 825, loss: 20355.582031\n",
            "Iteration: 850, loss: 20271.275391\n",
            "Iteration: 875, loss: 20201.261719\n",
            "Iteration: 900, loss: 20141.607422\n",
            "Iteration: 925, loss: 20089.808594\n",
            "Iteration: 950, loss: 20039.052734\n",
            "Iteration: 975, loss: 19995.044922\n",
            "Iteration: 1000, loss: 19954.191406\n",
            "Iteration: 25, loss: 47605528.000000\n",
            "Iteration: 50, loss: 46948488.000000\n",
            "Iteration: 75, loss: 46168832.000000\n",
            "Iteration: 100, loss: 45300760.000000\n",
            "Iteration: 125, loss: 44370720.000000\n",
            "Iteration: 150, loss: 43391456.000000\n",
            "Iteration: 175, loss: 42741656.000000\n",
            "Iteration: 200, loss: 42511032.000000\n",
            "Iteration: 225, loss: 42268320.000000\n",
            "Iteration: 250, loss: 42015440.000000\n",
            "Iteration: 275, loss: 41753932.000000\n",
            "Iteration: 300, loss: 41160152.000000\n",
            "Iteration: 325, loss: 40682500.000000\n",
            "Iteration: 350, loss: 8197062.000000\n",
            "Iteration: 375, loss: 2679701.250000\n",
            "Iteration: 400, loss: 1238720.500000\n",
            "Iteration: 425, loss: 690713.812500\n",
            "Iteration: 450, loss: 450506.312500\n",
            "Iteration: 475, loss: 344263.750000\n",
            "Iteration: 500, loss: 284867.843750\n",
            "Iteration: 525, loss: 246885.968750\n",
            "Iteration: 550, loss: 219955.156250\n",
            "Iteration: 575, loss: 199625.984375\n",
            "Iteration: 600, loss: 184869.468750\n",
            "Iteration: 625, loss: 172983.593750\n",
            "Iteration: 650, loss: 163593.625000\n",
            "Iteration: 675, loss: 155894.406250\n",
            "Iteration: 700, loss: 149576.562500\n",
            "Iteration: 725, loss: 144102.531250\n",
            "Iteration: 750, loss: 139800.562500\n",
            "Iteration: 775, loss: 135641.687500\n",
            "Iteration: 800, loss: 132300.875000\n",
            "Iteration: 825, loss: 129044.921875\n",
            "Iteration: 850, loss: 126085.898438\n",
            "Iteration: 875, loss: 123534.960938\n",
            "Iteration: 900, loss: 121074.804688\n",
            "Iteration: 925, loss: 118884.593750\n",
            "Iteration: 950, loss: 116910.664062\n",
            "Iteration: 975, loss: 115070.484375\n",
            "Iteration: 1000, loss: 113375.054688\n",
            "Iteration: 25, loss: 10094934.000000\n",
            "Iteration: 50, loss: 3359715.500000\n",
            "Iteration: 75, loss: 1769639.750000\n",
            "Iteration: 100, loss: 1022548.375000\n",
            "Iteration: 125, loss: 653723.500000\n",
            "Iteration: 150, loss: 452354.125000\n",
            "Iteration: 175, loss: 331214.687500\n",
            "Iteration: 200, loss: 260180.250000\n",
            "Iteration: 225, loss: 219718.187500\n",
            "Iteration: 250, loss: 192146.781250\n",
            "Iteration: 275, loss: 173334.390625\n",
            "Iteration: 300, loss: 159671.468750\n",
            "Iteration: 325, loss: 147831.593750\n",
            "Iteration: 350, loss: 138775.765625\n",
            "Iteration: 375, loss: 130947.804688\n",
            "Iteration: 400, loss: 124455.664062\n",
            "Iteration: 425, loss: 119151.640625\n",
            "Iteration: 450, loss: 114438.968750\n",
            "Iteration: 475, loss: 110355.898438\n",
            "Iteration: 500, loss: 106945.906250\n",
            "Iteration: 525, loss: 103635.757812\n",
            "Iteration: 550, loss: 100869.726562\n",
            "Iteration: 575, loss: 98321.156250\n",
            "Iteration: 600, loss: 96023.812500\n",
            "Iteration: 625, loss: 94066.828125\n",
            "Iteration: 650, loss: 92405.031250\n",
            "Iteration: 675, loss: 91012.648438\n",
            "Iteration: 700, loss: 89633.437500\n",
            "Iteration: 725, loss: 88444.343750\n",
            "Iteration: 750, loss: 87314.515625\n",
            "Iteration: 775, loss: 86347.289062\n",
            "Iteration: 800, loss: 85447.500000\n",
            "Iteration: 825, loss: 85324.375000\n",
            "Iteration: 850, loss: 84037.117188\n",
            "Iteration: 875, loss: 83259.593750\n",
            "Iteration: 900, loss: 82593.492188\n",
            "Iteration: 925, loss: 81865.476562\n",
            "Iteration: 950, loss: 81228.203125\n",
            "Iteration: 975, loss: 80960.437500\n",
            "Iteration: 1000, loss: 80194.843750\n",
            "Iteration: 25, loss: 3709859.750000\n",
            "Iteration: 50, loss: 1672885.250000\n",
            "Iteration: 75, loss: 938813.750000\n",
            "Iteration: 100, loss: 576802.312500\n",
            "Iteration: 125, loss: 412401.218750\n",
            "Iteration: 150, loss: 315631.031250\n",
            "Iteration: 175, loss: 259760.265625\n",
            "Iteration: 200, loss: 217079.562500\n",
            "Iteration: 225, loss: 186643.843750\n",
            "Iteration: 250, loss: 165572.406250\n",
            "Iteration: 275, loss: 149655.406250\n",
            "Iteration: 300, loss: 136658.140625\n",
            "Iteration: 325, loss: 127566.906250\n",
            "Iteration: 350, loss: 120156.703125\n",
            "Iteration: 375, loss: 114343.914062\n",
            "Iteration: 400, loss: 109806.062500\n",
            "Iteration: 425, loss: 105715.328125\n",
            "Iteration: 450, loss: 102160.953125\n",
            "Iteration: 475, loss: 98975.796875\n",
            "Iteration: 500, loss: 96111.398438\n",
            "Iteration: 525, loss: 93456.867188\n",
            "Iteration: 550, loss: 91281.250000\n",
            "Iteration: 575, loss: 89418.367188\n",
            "Iteration: 600, loss: 87770.109375\n",
            "Iteration: 625, loss: 86178.531250\n",
            "Iteration: 650, loss: 84695.117188\n",
            "Iteration: 675, loss: 83301.890625\n",
            "Iteration: 700, loss: 81895.640625\n",
            "Iteration: 725, loss: 80702.125000\n",
            "Iteration: 750, loss: 79538.445312\n",
            "Iteration: 775, loss: 78477.406250\n",
            "Iteration: 800, loss: 77438.281250\n",
            "Iteration: 825, loss: 76450.992188\n",
            "Iteration: 850, loss: 75574.937500\n",
            "Iteration: 875, loss: 74677.726562\n",
            "Iteration: 900, loss: 73887.507812\n",
            "Iteration: 925, loss: 73179.382812\n",
            "Iteration: 950, loss: 72535.007812\n",
            "Iteration: 975, loss: 71959.742188\n",
            "Iteration: 1000, loss: 71403.218750\n",
            "Iteration: 25, loss: 6668458.500000\n",
            "Iteration: 50, loss: 508390.968750\n",
            "Iteration: 75, loss: 204885.296875\n",
            "Iteration: 100, loss: 104537.734375\n",
            "Iteration: 125, loss: 82205.867188\n",
            "Iteration: 150, loss: 71634.312500\n",
            "Iteration: 175, loss: 65211.804688\n",
            "Iteration: 200, loss: 60884.472656\n",
            "Iteration: 225, loss: 57814.976562\n",
            "Iteration: 250, loss: 55663.980469\n",
            "Iteration: 275, loss: 54056.628906\n",
            "Iteration: 300, loss: 52781.722656\n",
            "Iteration: 325, loss: 51761.394531\n",
            "Iteration: 350, loss: 50904.269531\n",
            "Iteration: 375, loss: 50181.820312\n",
            "Iteration: 400, loss: 49529.382812\n",
            "Iteration: 425, loss: 48958.488281\n",
            "Iteration: 450, loss: 48442.523438\n",
            "Iteration: 475, loss: 48022.054688\n",
            "Iteration: 500, loss: 47645.816406\n",
            "Iteration: 525, loss: 47343.503906\n",
            "Iteration: 550, loss: 47056.742188\n",
            "Iteration: 575, loss: 46815.769531\n",
            "Iteration: 600, loss: 46591.953125\n",
            "Iteration: 625, loss: 46412.140625\n",
            "Iteration: 650, loss: 46246.289062\n",
            "Iteration: 675, loss: 46092.246094\n",
            "Iteration: 700, loss: 45956.046875\n",
            "Iteration: 725, loss: 45839.738281\n",
            "Iteration: 750, loss: 45742.953125\n",
            "Iteration: 775, loss: 45646.863281\n",
            "Iteration: 800, loss: 45559.546875\n",
            "Iteration: 825, loss: 45472.628906\n",
            "Iteration: 850, loss: 45397.761719\n",
            "Iteration: 875, loss: 45327.902344\n",
            "Iteration: 900, loss: 45262.085938\n",
            "Iteration: 925, loss: 45204.703125\n",
            "Iteration: 950, loss: 45153.351562\n",
            "Iteration: 975, loss: 45103.199219\n",
            "Iteration: 1000, loss: 45056.011719\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"random\", 1000, 25, output_dir+'random_avg_1000/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 96520.382812\n",
            "Iteration: 50, loss: 74410.859375\n",
            "Iteration: 75, loss: 66275.546875\n",
            "Iteration: 100, loss: 61591.773438\n",
            "Iteration: 125, loss: 58651.726562\n",
            "Iteration: 150, loss: 56344.597656\n",
            "Iteration: 175, loss: 54540.023438\n",
            "Iteration: 200, loss: 53070.164062\n",
            "Iteration: 225, loss: 51901.343750\n",
            "Iteration: 250, loss: 50851.824219\n",
            "Iteration: 275, loss: 49918.851562\n",
            "Iteration: 300, loss: 49102.035156\n",
            "Iteration: 325, loss: 48335.472656\n",
            "Iteration: 350, loss: 47653.390625\n",
            "Iteration: 375, loss: 47028.441406\n",
            "Iteration: 400, loss: 46486.664062\n",
            "Iteration: 425, loss: 45963.085938\n",
            "Iteration: 450, loss: 45491.394531\n",
            "Iteration: 475, loss: 45046.312500\n",
            "Iteration: 500, loss: 44628.023438\n",
            "Iteration: 525, loss: 44242.386719\n",
            "Iteration: 550, loss: 43887.632812\n",
            "Iteration: 575, loss: 43523.445312\n",
            "Iteration: 600, loss: 43191.878906\n",
            "Iteration: 625, loss: 42895.304688\n",
            "Iteration: 650, loss: 42629.984375\n",
            "Iteration: 675, loss: 42360.199219\n",
            "Iteration: 700, loss: 42119.359375\n",
            "Iteration: 725, loss: 41898.835938\n",
            "Iteration: 750, loss: 41705.996094\n",
            "Iteration: 775, loss: 41511.996094\n",
            "Iteration: 800, loss: 41339.898438\n",
            "Iteration: 825, loss: 41177.128906\n",
            "Iteration: 850, loss: 41028.640625\n",
            "Iteration: 875, loss: 40888.414062\n",
            "Iteration: 900, loss: 40768.921875\n",
            "Iteration: 925, loss: 40652.777344\n",
            "Iteration: 950, loss: 40548.878906\n",
            "Iteration: 975, loss: 40448.589844\n",
            "Iteration: 1000, loss: 40358.203125\n",
            "Iteration: 25, loss: 59477.601562\n",
            "Iteration: 50, loss: 46453.773438\n",
            "Iteration: 75, loss: 40670.589844\n",
            "Iteration: 100, loss: 37040.226562\n",
            "Iteration: 125, loss: 34583.058594\n",
            "Iteration: 150, loss: 32714.060547\n",
            "Iteration: 175, loss: 31011.365234\n",
            "Iteration: 200, loss: 29617.332031\n",
            "Iteration: 225, loss: 28443.001953\n",
            "Iteration: 250, loss: 27425.251953\n",
            "Iteration: 275, loss: 26542.328125\n",
            "Iteration: 300, loss: 25774.476562\n",
            "Iteration: 325, loss: 25083.763672\n",
            "Iteration: 350, loss: 24476.611328\n",
            "Iteration: 375, loss: 23952.160156\n",
            "Iteration: 400, loss: 23487.572266\n",
            "Iteration: 425, loss: 23074.966797\n",
            "Iteration: 450, loss: 22708.699219\n",
            "Iteration: 475, loss: 22371.580078\n",
            "Iteration: 500, loss: 22104.343750\n",
            "Iteration: 525, loss: 21850.976562\n",
            "Iteration: 550, loss: 21607.531250\n",
            "Iteration: 575, loss: 21394.347656\n",
            "Iteration: 600, loss: 21213.269531\n",
            "Iteration: 625, loss: 21041.316406\n",
            "Iteration: 650, loss: 20892.546875\n",
            "Iteration: 675, loss: 20754.376953\n",
            "Iteration: 700, loss: 20628.330078\n",
            "Iteration: 725, loss: 20507.931641\n",
            "Iteration: 750, loss: 20399.113281\n",
            "Iteration: 775, loss: 20295.611328\n",
            "Iteration: 800, loss: 20206.037109\n",
            "Iteration: 825, loss: 20123.605469\n",
            "Iteration: 850, loss: 20050.609375\n",
            "Iteration: 875, loss: 19977.734375\n",
            "Iteration: 900, loss: 19920.304688\n",
            "Iteration: 925, loss: 19870.740234\n",
            "Iteration: 950, loss: 19824.164062\n",
            "Iteration: 975, loss: 19779.339844\n",
            "Iteration: 1000, loss: 19735.523438\n",
            "Iteration: 25, loss: 245799.515625\n",
            "Iteration: 50, loss: 182308.734375\n",
            "Iteration: 75, loss: 163843.125000\n",
            "Iteration: 100, loss: 153348.281250\n",
            "Iteration: 125, loss: 145899.296875\n",
            "Iteration: 150, loss: 140597.125000\n",
            "Iteration: 175, loss: 136489.312500\n",
            "Iteration: 200, loss: 133049.031250\n",
            "Iteration: 225, loss: 130015.859375\n",
            "Iteration: 250, loss: 127484.281250\n",
            "Iteration: 275, loss: 125423.109375\n",
            "Iteration: 300, loss: 123417.023438\n",
            "Iteration: 325, loss: 121670.375000\n",
            "Iteration: 350, loss: 120048.234375\n",
            "Iteration: 375, loss: 118539.171875\n",
            "Iteration: 400, loss: 117054.367188\n",
            "Iteration: 425, loss: 115551.773438\n",
            "Iteration: 450, loss: 114268.085938\n",
            "Iteration: 475, loss: 113073.421875\n",
            "Iteration: 500, loss: 111921.875000\n",
            "Iteration: 525, loss: 110854.648438\n",
            "Iteration: 550, loss: 109863.476562\n",
            "Iteration: 575, loss: 108911.062500\n",
            "Iteration: 600, loss: 107983.031250\n",
            "Iteration: 625, loss: 107098.000000\n",
            "Iteration: 650, loss: 106234.593750\n",
            "Iteration: 675, loss: 105391.437500\n",
            "Iteration: 700, loss: 104609.031250\n",
            "Iteration: 725, loss: 103826.992188\n",
            "Iteration: 750, loss: 103112.835938\n",
            "Iteration: 775, loss: 102395.703125\n",
            "Iteration: 800, loss: 101656.515625\n",
            "Iteration: 825, loss: 100926.609375\n",
            "Iteration: 850, loss: 100188.953125\n",
            "Iteration: 875, loss: 99474.187500\n",
            "Iteration: 900, loss: 98790.437500\n",
            "Iteration: 925, loss: 98189.000000\n",
            "Iteration: 950, loss: 97608.414062\n",
            "Iteration: 975, loss: 97061.617188\n",
            "Iteration: 1000, loss: 96549.750000\n",
            "Iteration: 25, loss: 321523.281250\n",
            "Iteration: 50, loss: 168229.562500\n",
            "Iteration: 75, loss: 139542.171875\n",
            "Iteration: 100, loss: 120768.421875\n",
            "Iteration: 125, loss: 113248.406250\n",
            "Iteration: 150, loss: 107515.820312\n",
            "Iteration: 175, loss: 104176.921875\n",
            "Iteration: 200, loss: 101153.976562\n",
            "Iteration: 225, loss: 98607.812500\n",
            "Iteration: 250, loss: 96441.656250\n",
            "Iteration: 275, loss: 94651.828125\n",
            "Iteration: 300, loss: 92751.007812\n",
            "Iteration: 325, loss: 90938.609375\n",
            "Iteration: 350, loss: 89361.773438\n",
            "Iteration: 375, loss: 87883.445312\n",
            "Iteration: 400, loss: 86676.562500\n",
            "Iteration: 425, loss: 85501.398438\n",
            "Iteration: 450, loss: 84430.828125\n",
            "Iteration: 475, loss: 83363.031250\n",
            "Iteration: 500, loss: 82479.875000\n",
            "Iteration: 525, loss: 81563.382812\n",
            "Iteration: 550, loss: 80747.617188\n",
            "Iteration: 575, loss: 80026.046875\n",
            "Iteration: 600, loss: 79299.687500\n",
            "Iteration: 625, loss: 78678.921875\n",
            "Iteration: 650, loss: 78096.804688\n",
            "Iteration: 675, loss: 77456.578125\n",
            "Iteration: 700, loss: 76898.968750\n",
            "Iteration: 725, loss: 76374.976562\n",
            "Iteration: 750, loss: 75838.781250\n",
            "Iteration: 775, loss: 75342.976562\n",
            "Iteration: 800, loss: 74851.765625\n",
            "Iteration: 825, loss: 74443.140625\n",
            "Iteration: 850, loss: 74058.734375\n",
            "Iteration: 875, loss: 73670.414062\n",
            "Iteration: 900, loss: 73310.382812\n",
            "Iteration: 925, loss: 72936.984375\n",
            "Iteration: 950, loss: 72646.828125\n",
            "Iteration: 975, loss: 72325.007812\n",
            "Iteration: 1000, loss: 72015.359375\n",
            "Iteration: 25, loss: 145532.343750\n",
            "Iteration: 50, loss: 115028.343750\n",
            "Iteration: 75, loss: 104822.117188\n",
            "Iteration: 100, loss: 99545.679688\n",
            "Iteration: 125, loss: 95483.765625\n",
            "Iteration: 150, loss: 92226.507812\n",
            "Iteration: 175, loss: 89498.335938\n",
            "Iteration: 200, loss: 87421.250000\n",
            "Iteration: 225, loss: 85503.632812\n",
            "Iteration: 250, loss: 83863.007812\n",
            "Iteration: 275, loss: 82330.179688\n",
            "Iteration: 300, loss: 80951.468750\n",
            "Iteration: 325, loss: 79707.367188\n",
            "Iteration: 350, loss: 78505.546875\n",
            "Iteration: 375, loss: 77361.335938\n",
            "Iteration: 400, loss: 76360.765625\n",
            "Iteration: 425, loss: 75384.796875\n",
            "Iteration: 450, loss: 74477.359375\n",
            "Iteration: 475, loss: 73560.046875\n",
            "Iteration: 500, loss: 72766.109375\n",
            "Iteration: 525, loss: 72000.882812\n",
            "Iteration: 550, loss: 71239.390625\n",
            "Iteration: 575, loss: 70591.242188\n",
            "Iteration: 600, loss: 69907.484375\n",
            "Iteration: 625, loss: 69246.382812\n",
            "Iteration: 650, loss: 68667.289062\n",
            "Iteration: 675, loss: 68107.773438\n",
            "Iteration: 700, loss: 67587.898438\n",
            "Iteration: 725, loss: 67072.375000\n",
            "Iteration: 750, loss: 66556.500000\n",
            "Iteration: 775, loss: 66093.203125\n",
            "Iteration: 800, loss: 65654.882812\n",
            "Iteration: 825, loss: 65219.453125\n",
            "Iteration: 850, loss: 64814.378906\n",
            "Iteration: 875, loss: 64441.847656\n",
            "Iteration: 900, loss: 64084.105469\n",
            "Iteration: 925, loss: 63733.640625\n",
            "Iteration: 950, loss: 63371.566406\n",
            "Iteration: 975, loss: 63048.667969\n",
            "Iteration: 1000, loss: 62744.746094\n",
            "Iteration: 25, loss: 85743.492188\n",
            "Iteration: 50, loss: 69089.515625\n",
            "Iteration: 75, loss: 62292.210938\n",
            "Iteration: 100, loss: 58408.234375\n",
            "Iteration: 125, loss: 55890.753906\n",
            "Iteration: 150, loss: 54045.988281\n",
            "Iteration: 175, loss: 52674.207031\n",
            "Iteration: 200, loss: 51510.148438\n",
            "Iteration: 225, loss: 50575.710938\n",
            "Iteration: 250, loss: 49809.687500\n",
            "Iteration: 275, loss: 49154.359375\n",
            "Iteration: 300, loss: 48589.714844\n",
            "Iteration: 325, loss: 48101.496094\n",
            "Iteration: 350, loss: 47672.636719\n",
            "Iteration: 375, loss: 47306.792969\n",
            "Iteration: 400, loss: 46989.734375\n",
            "Iteration: 425, loss: 46722.453125\n",
            "Iteration: 450, loss: 46477.761719\n",
            "Iteration: 475, loss: 46271.230469\n",
            "Iteration: 500, loss: 46076.906250\n",
            "Iteration: 525, loss: 45915.339844\n",
            "Iteration: 550, loss: 45765.656250\n",
            "Iteration: 575, loss: 45628.921875\n",
            "Iteration: 600, loss: 45514.070312\n",
            "Iteration: 625, loss: 45411.746094\n",
            "Iteration: 650, loss: 45318.394531\n",
            "Iteration: 675, loss: 45234.996094\n",
            "Iteration: 700, loss: 45162.496094\n",
            "Iteration: 725, loss: 45093.804688\n",
            "Iteration: 750, loss: 45034.171875\n",
            "Iteration: 775, loss: 44978.027344\n",
            "Iteration: 800, loss: 44926.468750\n",
            "Iteration: 825, loss: 44879.980469\n",
            "Iteration: 850, loss: 44836.156250\n",
            "Iteration: 875, loss: 44797.699219\n",
            "Iteration: 900, loss: 44756.714844\n",
            "Iteration: 925, loss: 44723.273438\n",
            "Iteration: 950, loss: 44689.296875\n",
            "Iteration: 975, loss: 44658.941406\n",
            "Iteration: 1000, loss: 44631.355469\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"style\", 1000, 25, output_dir+'style_avg_1000/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 287508.437500\n",
            "Iteration: 50, loss: 108158.101562\n",
            "Iteration: 75, loss: 74132.367188\n",
            "Iteration: 100, loss: 62083.226562\n",
            "Iteration: 125, loss: 55868.820312\n",
            "Iteration: 150, loss: 52252.632812\n",
            "Iteration: 175, loss: 49653.625000\n",
            "Iteration: 200, loss: 47910.648438\n",
            "Iteration: 225, loss: 46472.140625\n",
            "Iteration: 250, loss: 45362.882812\n",
            "Iteration: 275, loss: 44510.957031\n",
            "Iteration: 300, loss: 43765.582031\n",
            "Iteration: 325, loss: 43184.031250\n",
            "Iteration: 350, loss: 42676.628906\n",
            "Iteration: 375, loss: 42219.832031\n",
            "Iteration: 400, loss: 41836.058594\n",
            "Iteration: 425, loss: 41505.632812\n",
            "Iteration: 450, loss: 41223.644531\n",
            "Iteration: 475, loss: 40960.730469\n",
            "Iteration: 500, loss: 40733.550781\n",
            "Iteration: 25, loss: 650821.937500\n",
            "Iteration: 50, loss: 167385.515625\n",
            "Iteration: 75, loss: 69997.554688\n",
            "Iteration: 100, loss: 45269.164062\n",
            "Iteration: 125, loss: 35320.769531\n",
            "Iteration: 150, loss: 31038.808594\n",
            "Iteration: 175, loss: 28237.648438\n",
            "Iteration: 200, loss: 26388.316406\n",
            "Iteration: 225, loss: 25076.755859\n",
            "Iteration: 250, loss: 23961.916016\n",
            "Iteration: 275, loss: 23133.089844\n",
            "Iteration: 300, loss: 22495.503906\n",
            "Iteration: 325, loss: 21959.910156\n",
            "Iteration: 350, loss: 21535.642578\n",
            "Iteration: 375, loss: 21186.492188\n",
            "Iteration: 400, loss: 20909.628906\n",
            "Iteration: 425, loss: 20683.474609\n",
            "Iteration: 450, loss: 20486.287109\n",
            "Iteration: 475, loss: 20321.361328\n",
            "Iteration: 500, loss: 20183.324219\n",
            "Iteration: 25, loss: 1559803.500000\n",
            "Iteration: 50, loss: 467063.125000\n",
            "Iteration: 75, loss: 252972.031250\n",
            "Iteration: 100, loss: 185576.343750\n",
            "Iteration: 125, loss: 150579.812500\n",
            "Iteration: 150, loss: 131692.421875\n",
            "Iteration: 175, loss: 119699.757812\n",
            "Iteration: 200, loss: 111510.265625\n",
            "Iteration: 225, loss: 105664.718750\n",
            "Iteration: 250, loss: 101461.742188\n",
            "Iteration: 275, loss: 98022.242188\n",
            "Iteration: 300, loss: 95250.039062\n",
            "Iteration: 325, loss: 92883.562500\n",
            "Iteration: 350, loss: 90828.773438\n",
            "Iteration: 375, loss: 89021.000000\n",
            "Iteration: 400, loss: 87482.515625\n",
            "Iteration: 425, loss: 86114.531250\n",
            "Iteration: 450, loss: 84955.234375\n",
            "Iteration: 475, loss: 83935.671875\n",
            "Iteration: 500, loss: 83005.484375\n",
            "Iteration: 25, loss: 3398538.500000\n",
            "Iteration: 50, loss: 819938.062500\n",
            "Iteration: 75, loss: 313214.312500\n",
            "Iteration: 100, loss: 214601.781250\n",
            "Iteration: 125, loss: 165630.875000\n",
            "Iteration: 150, loss: 134784.062500\n",
            "Iteration: 175, loss: 117871.929688\n",
            "Iteration: 200, loss: 110722.406250\n",
            "Iteration: 225, loss: 98449.125000\n",
            "Iteration: 250, loss: 92389.335938\n",
            "Iteration: 275, loss: 87086.750000\n",
            "Iteration: 300, loss: 82942.703125\n",
            "Iteration: 325, loss: 80265.750000\n",
            "Iteration: 350, loss: 77448.492188\n",
            "Iteration: 375, loss: 75226.421875\n",
            "Iteration: 400, loss: 73374.023438\n",
            "Iteration: 425, loss: 71887.703125\n",
            "Iteration: 450, loss: 70457.375000\n",
            "Iteration: 475, loss: 69209.664062\n",
            "Iteration: 500, loss: 68127.656250\n",
            "Iteration: 25, loss: 627305.437500\n",
            "Iteration: 50, loss: 270034.875000\n",
            "Iteration: 75, loss: 177416.968750\n",
            "Iteration: 100, loss: 129203.570312\n",
            "Iteration: 125, loss: 106852.625000\n",
            "Iteration: 150, loss: 92812.921875\n",
            "Iteration: 175, loss: 84118.421875\n",
            "Iteration: 200, loss: 77354.125000\n",
            "Iteration: 225, loss: 73090.468750\n",
            "Iteration: 250, loss: 69691.039062\n",
            "Iteration: 275, loss: 67261.296875\n",
            "Iteration: 300, loss: 65200.011719\n",
            "Iteration: 325, loss: 63413.187500\n",
            "Iteration: 350, loss: 61827.687500\n",
            "Iteration: 375, loss: 60554.527344\n",
            "Iteration: 400, loss: 59448.171875\n",
            "Iteration: 425, loss: 58402.109375\n",
            "Iteration: 450, loss: 57518.199219\n",
            "Iteration: 475, loss: 56751.527344\n",
            "Iteration: 500, loss: 56067.351562\n",
            "Iteration: 25, loss: 673285.687500\n",
            "Iteration: 50, loss: 167186.796875\n",
            "Iteration: 75, loss: 97297.132812\n",
            "Iteration: 100, loss: 75010.257812\n",
            "Iteration: 125, loss: 65639.875000\n",
            "Iteration: 150, loss: 60853.636719\n",
            "Iteration: 175, loss: 57816.082031\n",
            "Iteration: 200, loss: 55561.046875\n",
            "Iteration: 225, loss: 53762.296875\n",
            "Iteration: 250, loss: 52303.105469\n",
            "Iteration: 275, loss: 51051.859375\n",
            "Iteration: 300, loss: 50045.281250\n",
            "Iteration: 325, loss: 49218.175781\n",
            "Iteration: 350, loss: 48518.425781\n",
            "Iteration: 375, loss: 47990.089844\n",
            "Iteration: 400, loss: 47513.398438\n",
            "Iteration: 425, loss: 47108.875000\n",
            "Iteration: 450, loss: 46772.007812\n",
            "Iteration: 475, loss: 46502.609375\n",
            "Iteration: 500, loss: 46258.375000\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"content\", 500, 25, output_dir+'content_avg_500/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 1727825.125000\n",
            "Iteration: 50, loss: 319189.812500\n",
            "Iteration: 75, loss: 185153.968750\n",
            "Iteration: 100, loss: 140562.203125\n",
            "Iteration: 125, loss: 111091.414062\n",
            "Iteration: 150, loss: 92322.335938\n",
            "Iteration: 175, loss: 80028.000000\n",
            "Iteration: 200, loss: 70713.328125\n",
            "Iteration: 225, loss: 64488.789062\n",
            "Iteration: 250, loss: 60177.429688\n",
            "Iteration: 275, loss: 57077.539062\n",
            "Iteration: 300, loss: 54906.398438\n",
            "Iteration: 325, loss: 53103.628906\n",
            "Iteration: 350, loss: 51663.835938\n",
            "Iteration: 375, loss: 50434.261719\n",
            "Iteration: 400, loss: 49376.378906\n",
            "Iteration: 425, loss: 48424.132812\n",
            "Iteration: 450, loss: 47599.761719\n",
            "Iteration: 475, loss: 46912.851562\n",
            "Iteration: 500, loss: 46285.855469\n",
            "Iteration: 25, loss: 5024603.500000\n",
            "Iteration: 50, loss: 1085884.250000\n",
            "Iteration: 75, loss: 300514.031250\n",
            "Iteration: 100, loss: 110128.179688\n",
            "Iteration: 125, loss: 68359.875000\n",
            "Iteration: 150, loss: 51855.480469\n",
            "Iteration: 175, loss: 43959.199219\n",
            "Iteration: 200, loss: 38781.156250\n",
            "Iteration: 225, loss: 35303.722656\n",
            "Iteration: 250, loss: 32585.789062\n",
            "Iteration: 275, loss: 30649.126953\n",
            "Iteration: 300, loss: 29043.283203\n",
            "Iteration: 325, loss: 27694.597656\n",
            "Iteration: 350, loss: 26626.181641\n",
            "Iteration: 375, loss: 25699.337891\n",
            "Iteration: 400, loss: 24987.593750\n",
            "Iteration: 425, loss: 24320.242188\n",
            "Iteration: 450, loss: 23759.890625\n",
            "Iteration: 475, loss: 23272.984375\n",
            "Iteration: 500, loss: 22841.292969\n",
            "Iteration: 25, loss: 7706214.000000\n",
            "Iteration: 50, loss: 2435296.500000\n",
            "Iteration: 75, loss: 1120723.875000\n",
            "Iteration: 100, loss: 613051.125000\n",
            "Iteration: 125, loss: 423373.312500\n",
            "Iteration: 150, loss: 332702.125000\n",
            "Iteration: 175, loss: 278108.343750\n",
            "Iteration: 200, loss: 242851.375000\n",
            "Iteration: 225, loss: 218876.734375\n",
            "Iteration: 250, loss: 200892.921875\n",
            "Iteration: 275, loss: 187294.703125\n",
            "Iteration: 300, loss: 176135.812500\n",
            "Iteration: 325, loss: 167708.015625\n",
            "Iteration: 350, loss: 160298.171875\n",
            "Iteration: 375, loss: 154279.187500\n",
            "Iteration: 400, loss: 148835.031250\n",
            "Iteration: 425, loss: 144016.140625\n",
            "Iteration: 450, loss: 139633.625000\n",
            "Iteration: 475, loss: 135830.281250\n",
            "Iteration: 500, loss: 132293.359375\n",
            "Iteration: 25, loss: 7331058.000000\n",
            "Iteration: 50, loss: 3012086.250000\n",
            "Iteration: 75, loss: 1591935.500000\n",
            "Iteration: 100, loss: 977629.500000\n",
            "Iteration: 125, loss: 654651.250000\n",
            "Iteration: 150, loss: 459019.437500\n",
            "Iteration: 175, loss: 337477.968750\n",
            "Iteration: 200, loss: 263724.750000\n",
            "Iteration: 225, loss: 219985.250000\n",
            "Iteration: 250, loss: 193937.125000\n",
            "Iteration: 275, loss: 174122.328125\n",
            "Iteration: 300, loss: 160541.578125\n",
            "Iteration: 325, loss: 148535.328125\n",
            "Iteration: 350, loss: 139355.859375\n",
            "Iteration: 375, loss: 132311.609375\n",
            "Iteration: 400, loss: 125913.625000\n",
            "Iteration: 425, loss: 120305.468750\n",
            "Iteration: 450, loss: 115794.375000\n",
            "Iteration: 475, loss: 111418.070312\n",
            "Iteration: 500, loss: 107743.664062\n",
            "Iteration: 25, loss: 3000636.000000\n",
            "Iteration: 50, loss: 1511020.500000\n",
            "Iteration: 75, loss: 834920.625000\n",
            "Iteration: 100, loss: 535822.375000\n",
            "Iteration: 125, loss: 384206.687500\n",
            "Iteration: 150, loss: 301178.875000\n",
            "Iteration: 175, loss: 243576.734375\n",
            "Iteration: 200, loss: 199681.343750\n",
            "Iteration: 225, loss: 172267.250000\n",
            "Iteration: 250, loss: 151781.031250\n",
            "Iteration: 275, loss: 135656.625000\n",
            "Iteration: 300, loss: 123392.765625\n",
            "Iteration: 325, loss: 115919.359375\n",
            "Iteration: 350, loss: 109831.906250\n",
            "Iteration: 375, loss: 105191.546875\n",
            "Iteration: 400, loss: 101428.023438\n",
            "Iteration: 425, loss: 98314.812500\n",
            "Iteration: 450, loss: 95578.132812\n",
            "Iteration: 475, loss: 93237.671875\n",
            "Iteration: 500, loss: 91146.343750\n",
            "Iteration: 25, loss: 2613305.250000\n",
            "Iteration: 50, loss: 298429.000000\n",
            "Iteration: 75, loss: 136067.406250\n",
            "Iteration: 100, loss: 93407.421875\n",
            "Iteration: 125, loss: 77136.343750\n",
            "Iteration: 150, loss: 68965.015625\n",
            "Iteration: 175, loss: 63344.167969\n",
            "Iteration: 200, loss: 59634.234375\n",
            "Iteration: 225, loss: 56978.945312\n",
            "Iteration: 250, loss: 55110.050781\n",
            "Iteration: 275, loss: 53618.937500\n",
            "Iteration: 300, loss: 52477.921875\n",
            "Iteration: 325, loss: 51501.511719\n",
            "Iteration: 350, loss: 50705.515625\n",
            "Iteration: 375, loss: 50013.015625\n",
            "Iteration: 400, loss: 49415.335938\n",
            "Iteration: 425, loss: 48867.382812\n",
            "Iteration: 450, loss: 48416.281250\n",
            "Iteration: 475, loss: 48000.531250\n",
            "Iteration: 500, loss: 47623.667969\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"random\", 500, 25, output_dir+'random_avg_500/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 95415.640625\n",
            "Iteration: 50, loss: 73840.671875\n",
            "Iteration: 75, loss: 66170.375000\n",
            "Iteration: 100, loss: 61993.820312\n",
            "Iteration: 125, loss: 58930.878906\n",
            "Iteration: 150, loss: 56598.414062\n",
            "Iteration: 175, loss: 54687.515625\n",
            "Iteration: 200, loss: 53240.140625\n",
            "Iteration: 225, loss: 51993.562500\n",
            "Iteration: 250, loss: 50905.023438\n",
            "Iteration: 275, loss: 49972.125000\n",
            "Iteration: 300, loss: 49143.343750\n",
            "Iteration: 325, loss: 48375.289062\n",
            "Iteration: 350, loss: 47726.152344\n",
            "Iteration: 375, loss: 47098.156250\n",
            "Iteration: 400, loss: 46554.281250\n",
            "Iteration: 425, loss: 46025.976562\n",
            "Iteration: 450, loss: 45518.343750\n",
            "Iteration: 475, loss: 45083.855469\n",
            "Iteration: 500, loss: 44658.312500\n",
            "Iteration: 25, loss: 59236.136719\n",
            "Iteration: 50, loss: 46061.492188\n",
            "Iteration: 75, loss: 40275.054688\n",
            "Iteration: 100, loss: 36916.195312\n",
            "Iteration: 125, loss: 34465.023438\n",
            "Iteration: 150, loss: 32571.359375\n",
            "Iteration: 175, loss: 30932.566406\n",
            "Iteration: 200, loss: 29549.421875\n",
            "Iteration: 225, loss: 28387.931641\n",
            "Iteration: 250, loss: 27360.460938\n",
            "Iteration: 275, loss: 26493.566406\n",
            "Iteration: 300, loss: 25683.894531\n",
            "Iteration: 325, loss: 24997.037109\n",
            "Iteration: 350, loss: 24392.578125\n",
            "Iteration: 375, loss: 23865.253906\n",
            "Iteration: 400, loss: 23432.777344\n",
            "Iteration: 425, loss: 23029.203125\n",
            "Iteration: 450, loss: 22658.738281\n",
            "Iteration: 475, loss: 22346.824219\n",
            "Iteration: 500, loss: 22069.189453\n",
            "Iteration: 25, loss: 246104.218750\n",
            "Iteration: 50, loss: 182829.515625\n",
            "Iteration: 75, loss: 163942.796875\n",
            "Iteration: 100, loss: 153549.890625\n",
            "Iteration: 125, loss: 146435.609375\n",
            "Iteration: 150, loss: 140692.500000\n",
            "Iteration: 175, loss: 136566.562500\n",
            "Iteration: 200, loss: 133211.609375\n",
            "Iteration: 225, loss: 130331.617188\n",
            "Iteration: 250, loss: 127902.265625\n",
            "Iteration: 275, loss: 125661.968750\n",
            "Iteration: 300, loss: 123695.609375\n",
            "Iteration: 325, loss: 121857.132812\n",
            "Iteration: 350, loss: 120193.031250\n",
            "Iteration: 375, loss: 118691.898438\n",
            "Iteration: 400, loss: 117258.406250\n",
            "Iteration: 425, loss: 115835.320312\n",
            "Iteration: 450, loss: 114531.742188\n",
            "Iteration: 475, loss: 113345.015625\n",
            "Iteration: 500, loss: 112205.773438\n",
            "Iteration: 25, loss: 317276.187500\n",
            "Iteration: 50, loss: 168065.500000\n",
            "Iteration: 75, loss: 138560.234375\n",
            "Iteration: 100, loss: 120264.726562\n",
            "Iteration: 125, loss: 112423.593750\n",
            "Iteration: 150, loss: 107530.773438\n",
            "Iteration: 175, loss: 104395.859375\n",
            "Iteration: 200, loss: 101034.726562\n",
            "Iteration: 225, loss: 98379.437500\n",
            "Iteration: 250, loss: 96034.140625\n",
            "Iteration: 275, loss: 94260.554688\n",
            "Iteration: 300, loss: 92176.820312\n",
            "Iteration: 325, loss: 90508.203125\n",
            "Iteration: 350, loss: 88795.492188\n",
            "Iteration: 375, loss: 87408.093750\n",
            "Iteration: 400, loss: 86070.703125\n",
            "Iteration: 425, loss: 85082.476562\n",
            "Iteration: 450, loss: 83950.156250\n",
            "Iteration: 475, loss: 83003.679688\n",
            "Iteration: 500, loss: 82093.546875\n",
            "Iteration: 25, loss: 145169.312500\n",
            "Iteration: 50, loss: 116801.054688\n",
            "Iteration: 75, loss: 104596.343750\n",
            "Iteration: 100, loss: 98825.804688\n",
            "Iteration: 125, loss: 95193.593750\n",
            "Iteration: 150, loss: 92767.328125\n",
            "Iteration: 175, loss: 90059.507812\n",
            "Iteration: 200, loss: 87789.757812\n",
            "Iteration: 225, loss: 85964.406250\n",
            "Iteration: 250, loss: 84226.125000\n",
            "Iteration: 275, loss: 82647.226562\n",
            "Iteration: 300, loss: 81249.007812\n",
            "Iteration: 325, loss: 80073.820312\n",
            "Iteration: 350, loss: 78816.046875\n",
            "Iteration: 375, loss: 77739.484375\n",
            "Iteration: 400, loss: 76773.734375\n",
            "Iteration: 425, loss: 75765.898438\n",
            "Iteration: 450, loss: 74816.343750\n",
            "Iteration: 475, loss: 73922.281250\n",
            "Iteration: 500, loss: 73152.101562\n",
            "Iteration: 25, loss: 85770.117188\n",
            "Iteration: 50, loss: 69099.765625\n",
            "Iteration: 75, loss: 62458.691406\n",
            "Iteration: 100, loss: 58486.902344\n",
            "Iteration: 125, loss: 55996.097656\n",
            "Iteration: 150, loss: 54113.726562\n",
            "Iteration: 175, loss: 52732.410156\n",
            "Iteration: 200, loss: 51622.308594\n",
            "Iteration: 225, loss: 50688.640625\n",
            "Iteration: 250, loss: 49928.929688\n",
            "Iteration: 275, loss: 49260.234375\n",
            "Iteration: 300, loss: 48655.750000\n",
            "Iteration: 325, loss: 48155.890625\n",
            "Iteration: 350, loss: 47711.453125\n",
            "Iteration: 375, loss: 47336.238281\n",
            "Iteration: 400, loss: 47008.609375\n",
            "Iteration: 425, loss: 46733.214844\n",
            "Iteration: 450, loss: 46484.605469\n",
            "Iteration: 475, loss: 46269.367188\n",
            "Iteration: 500, loss: 46090.152344\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"style\", 500, 25, output_dir+'style_avg_500/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 301201.937500\n",
            "Iteration: 50, loss: 115656.546875\n",
            "Iteration: 75, loss: 74828.117188\n",
            "Iteration: 100, loss: 62137.597656\n",
            "Iteration: 125, loss: 55887.371094\n",
            "Iteration: 150, loss: 52183.687500\n",
            "Iteration: 175, loss: 49706.992188\n",
            "Iteration: 200, loss: 47919.421875\n",
            "Iteration: 25, loss: 644999.812500\n",
            "Iteration: 50, loss: 187304.406250\n",
            "Iteration: 75, loss: 72648.265625\n",
            "Iteration: 100, loss: 45187.433594\n",
            "Iteration: 125, loss: 35425.578125\n",
            "Iteration: 150, loss: 30884.177734\n",
            "Iteration: 175, loss: 28180.919922\n",
            "Iteration: 200, loss: 26299.113281\n",
            "Iteration: 25, loss: 1520102.750000\n",
            "Iteration: 50, loss: 472436.625000\n",
            "Iteration: 75, loss: 254700.671875\n",
            "Iteration: 100, loss: 183625.546875\n",
            "Iteration: 125, loss: 149063.406250\n",
            "Iteration: 150, loss: 129886.812500\n",
            "Iteration: 175, loss: 118479.843750\n",
            "Iteration: 200, loss: 110894.546875\n",
            "Iteration: 25, loss: 3419820.000000\n",
            "Iteration: 50, loss: 812831.750000\n",
            "Iteration: 75, loss: 323534.406250\n",
            "Iteration: 100, loss: 217471.718750\n",
            "Iteration: 125, loss: 171840.781250\n",
            "Iteration: 150, loss: 139064.328125\n",
            "Iteration: 175, loss: 120439.875000\n",
            "Iteration: 200, loss: 109406.437500\n",
            "Iteration: 25, loss: 622541.125000\n",
            "Iteration: 50, loss: 270097.468750\n",
            "Iteration: 75, loss: 177153.156250\n",
            "Iteration: 100, loss: 128456.156250\n",
            "Iteration: 125, loss: 106026.234375\n",
            "Iteration: 150, loss: 92127.804688\n",
            "Iteration: 175, loss: 83435.976562\n",
            "Iteration: 200, loss: 77051.843750\n",
            "Iteration: 25, loss: 653564.500000\n",
            "Iteration: 50, loss: 486862.750000\n",
            "Iteration: 75, loss: 95444.125000\n",
            "Iteration: 100, loss: 73752.335938\n",
            "Iteration: 125, loss: 64986.726562\n",
            "Iteration: 150, loss: 60487.437500\n",
            "Iteration: 175, loss: 57482.789062\n",
            "Iteration: 200, loss: 55394.710938\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"content\", 200, 25, output_dir+'content_avg_200/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 1227793.000000\n",
            "Iteration: 50, loss: 309846.281250\n",
            "Iteration: 75, loss: 184797.093750\n",
            "Iteration: 100, loss: 138633.781250\n",
            "Iteration: 125, loss: 110505.742188\n",
            "Iteration: 150, loss: 92106.070312\n",
            "Iteration: 175, loss: 79604.593750\n",
            "Iteration: 200, loss: 70428.375000\n",
            "Iteration: 25, loss: 1973927.000000\n",
            "Iteration: 50, loss: 355916.750000\n",
            "Iteration: 75, loss: 117871.289062\n",
            "Iteration: 100, loss: 67081.007812\n",
            "Iteration: 125, loss: 51838.445312\n",
            "Iteration: 150, loss: 43749.175781\n",
            "Iteration: 175, loss: 38793.335938\n",
            "Iteration: 200, loss: 35105.738281\n",
            "Iteration: 25, loss: 8067645.000000\n",
            "Iteration: 50, loss: 2401661.500000\n",
            "Iteration: 75, loss: 1129857.750000\n",
            "Iteration: 100, loss: 650713.750000\n",
            "Iteration: 125, loss: 439746.312500\n",
            "Iteration: 150, loss: 336653.843750\n",
            "Iteration: 175, loss: 278652.906250\n",
            "Iteration: 200, loss: 240302.359375\n",
            "Iteration: 25, loss: 11340284.000000\n",
            "Iteration: 50, loss: 3625001.000000\n",
            "Iteration: 75, loss: 1925423.625000\n",
            "Iteration: 100, loss: 1173195.875000\n",
            "Iteration: 125, loss: 748528.375000\n",
            "Iteration: 150, loss: 500669.937500\n",
            "Iteration: 175, loss: 359723.375000\n",
            "Iteration: 200, loss: 277375.437500\n",
            "Iteration: 25, loss: 5811724.500000\n",
            "Iteration: 50, loss: 1891130.500000\n",
            "Iteration: 75, loss: 985972.500000\n",
            "Iteration: 100, loss: 578956.000000\n",
            "Iteration: 125, loss: 403651.843750\n",
            "Iteration: 150, loss: 314563.625000\n",
            "Iteration: 175, loss: 251235.781250\n",
            "Iteration: 200, loss: 211617.406250\n",
            "Iteration: 25, loss: 2577204.250000\n",
            "Iteration: 50, loss: 311567.625000\n",
            "Iteration: 75, loss: 143626.234375\n",
            "Iteration: 100, loss: 95386.203125\n",
            "Iteration: 125, loss: 78040.765625\n",
            "Iteration: 150, loss: 69640.757812\n",
            "Iteration: 175, loss: 63704.117188\n",
            "Iteration: 200, loss: 59875.796875\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"random\", 200, 25, output_dir+'random_avg_200/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 25, loss: 96676.007812\n",
            "Iteration: 50, loss: 73740.617188\n",
            "Iteration: 75, loss: 65713.015625\n",
            "Iteration: 100, loss: 61632.195312\n",
            "Iteration: 125, loss: 58495.992188\n",
            "Iteration: 150, loss: 56368.664062\n",
            "Iteration: 175, loss: 54574.476562\n",
            "Iteration: 200, loss: 53202.589844\n",
            "Iteration: 25, loss: 59255.566406\n",
            "Iteration: 50, loss: 46147.460938\n",
            "Iteration: 75, loss: 40241.042969\n",
            "Iteration: 100, loss: 36933.093750\n",
            "Iteration: 125, loss: 34421.335938\n",
            "Iteration: 150, loss: 32495.177734\n",
            "Iteration: 175, loss: 30857.234375\n",
            "Iteration: 200, loss: 29480.449219\n",
            "Iteration: 25, loss: 243979.156250\n",
            "Iteration: 50, loss: 185001.859375\n",
            "Iteration: 75, loss: 165537.546875\n",
            "Iteration: 100, loss: 154063.796875\n",
            "Iteration: 125, loss: 146851.890625\n",
            "Iteration: 150, loss: 141234.218750\n",
            "Iteration: 175, loss: 137018.984375\n",
            "Iteration: 200, loss: 133427.609375\n",
            "Iteration: 25, loss: 318558.187500\n",
            "Iteration: 50, loss: 166989.359375\n",
            "Iteration: 75, loss: 139686.750000\n",
            "Iteration: 100, loss: 123191.359375\n",
            "Iteration: 125, loss: 115529.914062\n",
            "Iteration: 150, loss: 109248.531250\n",
            "Iteration: 175, loss: 104740.296875\n",
            "Iteration: 200, loss: 101562.671875\n",
            "Iteration: 25, loss: 145145.468750\n",
            "Iteration: 50, loss: 114632.195312\n",
            "Iteration: 75, loss: 105126.468750\n",
            "Iteration: 100, loss: 99791.039062\n",
            "Iteration: 125, loss: 95815.679688\n",
            "Iteration: 150, loss: 92592.015625\n",
            "Iteration: 175, loss: 89984.578125\n",
            "Iteration: 200, loss: 87745.734375\n",
            "Iteration: 25, loss: 85827.312500\n",
            "Iteration: 50, loss: 69203.859375\n",
            "Iteration: 75, loss: 62412.296875\n",
            "Iteration: 100, loss: 58600.625000\n",
            "Iteration: 125, loss: 55979.699219\n",
            "Iteration: 150, loss: 54235.914062\n",
            "Iteration: 175, loss: 52837.878906\n",
            "Iteration: 200, loss: 51691.500000\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(style_images)):\n",
        "    run_transfer(style_images[i], content_images[i], \"style\", 200, 25, output_dir+'style_avg_200/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test block, will eventually remove it\n",
        "#temp1 = Image.open(image_dir+style_name)\n",
        "#img_resize = transforms.Resize((temp.height, temp.width))\n",
        "#temp_resized = img_resize(temp1)\n",
        "#plt.imshow(temp_resized)\n",
        "#temp_resized.save(output_dir + \"c\" + content_name[8:10] + \"_s\" + style_name[6:8] + \".png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NeuralStyleTransfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
